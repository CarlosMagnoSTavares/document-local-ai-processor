# ü§ñ CORRE√á√ïES IMPLEMENTADAS - ERRO 404 OLLAMA

## üìã PROBLEMA IDENTIFICADO

**Sintomas**: 
- API retorna erro 404 para `http://localhost:11434/api/generate`
- Uploads falham com mensagem de Ollama n√£o encontrado
- Container parece estar rodando mas Ollama n√£o responde

**Causa Raiz**: 
- Configura√ß√£o inadequada de vari√°veis de ambiente do Ollama
- Problemas na ordem de inicializa√ß√£o dos servi√ßos
- Timeout insuficiente para startup do Ollama
- Falta de verifica√ß√£o de conectividade

## üîß CORRE√á√ïES IMPLEMENTADAS

### 1. **docker-compose.yml** - Melhorias na Configura√ß√£o
```diff
+ ports:
+   - "11434:11434"  # Ollama API
+ volumes:
+   - ollama_data:/root/.ollama  # Persistir dados do Ollama
+ environment:
+   - OLLAMA_HOST=0.0.0.0:11434
+   - OLLAMA_DEBUG=1
+ healthcheck:
+   test: ["CMD", "curl", "-f", "http://localhost:8000/health || curl -f http://localhost:11434/api/tags"]
+   start_period: 60s

+ volumes:
+   ollama_data:
```

### 2. **supervisord.conf** - Prioridades e Configura√ß√£o Avan√ßada
```diff
[program:ollama]
+ environment=OLLAMA_HOST=0.0.0.0:11434,OLLAMA_DEBUG=1,OLLAMA_VERBOSE=1,OLLAMA_ORIGINS=*
+ priority=100

[program:redis]
+ priority=50

[program:fastapi]
+ priority=200

[program:celery_worker]
+ priority=300
```

### 3. **start.sh** - Verifica√ß√£o Robusta de Conectividade
```diff
# Start Ollama in background and pull the model
echo "ü§ñ Starting Ollama and pulling model..."
+ export OLLAMA_HOST=0.0.0.0:11434
+ export OLLAMA_DEBUG=1
+ export OLLAMA_ORIGINS=*

ollama serve &
OLLAMA_PID=$!

# Wait for Ollama to be ready with retry logic
+ RETRY_COUNT=0
+ MAX_RETRIES=30
+ while ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; do
+     if [ $RETRY_COUNT -ge $MAX_RETRIES ]; then
+         echo "‚ùå Ollama failed to start after $MAX_RETRIES attempts"
+         break
+     fi
+     echo "üîÑ Attempt $((RETRY_COUNT + 1))/$MAX_RETRIES - Waiting for Ollama..."
+     sleep 2
+     RETRY_COUNT=$((RETRY_COUNT + 1))
+ done

+ if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
+     echo "‚úÖ Ollama is ready!"
+ fi
```

### 4. **workers.py** - Verifica√ß√£o de Conectividade antes do Processamento
```diff
else:
    logger.info(f"üè† VERBOSE: Using Ollama (Local)")
    
+   # Verificar se Ollama est√° dispon√≠vel
+   try:
+       import httpx
+       async def check_ollama():
+           async with httpx.AsyncClient(timeout=5) as client:
+               response = await client.get(f"{OLLAMA_BASE_URL}/api/tags")
+               return response.status_code == 200
+       
+       ollama_available = loop.run_until_complete(check_ollama())
+       
+       if not ollama_available:
+           logger.error(f"‚ùå CRITICAL: Ollama not available at {OLLAMA_BASE_URL}")
+           raise Exception(f"Ollama service not available at {OLLAMA_BASE_URL}")
+           
+   except Exception as connectivity_error:
+       logger.error(f"‚ùå CRITICAL: Failed to connect to Ollama: {connectivity_error}")
+       raise Exception(f"Ollama connectivity error: {connectivity_error}")
```

### 5. **.env.example** - Configura√ß√µes Adequadas
```diff
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
+ OLLAMA_HOST=0.0.0.0:11434
+ OLLAMA_DEBUG=1
+ OLLAMA_ORIGINS=*
```

## üõ†Ô∏è FERRAMENTAS DE DEBUG CRIADAS

### **check_services.bat** (Windows)
- Verifica√ß√£o r√°pida de todos os servi√ßos
- Teste de conectividade Ollama
- Status dos containers
- Logs recentes

### **rebuild_and_debug.bat** (Windows)
- Rebuild completo com cache limpo
- Aguarda inicializa√ß√£o adequada
- Testa conectividade automaticamente
- Fornece comandos de debug

### **check_services.sh** (Linux/Mac)
- Vers√£o Unix dos scripts de verifica√ß√£o

### **rebuild_and_debug.sh** (Linux/Mac)
- Vers√£o Unix do rebuild automatizado

## üöÄ INSTRU√á√ïES DE USO

### **Para resolver o erro 404 imediatamente:**

1. **Verifica√ß√£o r√°pida**:
```bash
# Windows
check_services.bat

# Linux/Mac
./check_services.sh
```

2. **Se Ollama n√£o responder, rebuild completo**:
```bash
# Windows
rebuild_and_debug.bat

# Linux/Mac
./rebuild_and_debug.sh
```

3. **Verifica√ß√£o manual**:
```bash
# Testar Ollama diretamente
curl http://localhost:11434/api/tags

# Ver logs espec√≠ficos
docker-compose logs document-ocr-api | grep -E "(ollama|Ollama)"

# Entrar no container para debug
docker exec -it $(docker-compose ps -q) bash
```

## üìä TROUBLESHOOTING AVAN√áADO

### **Se o problema persistir:**

1. **Verificar se Docker est√° funcionando**:
```bash
docker --version
docker-compose --version
```

2. **Verificar portas ocupadas**:
```bash
# Windows
netstat -an | findstr 11434

# Linux/Mac
netstat -tulpn | grep 11434
```

3. **Usar Gemini como alternativa**:
```bash
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "AI-Provider: gemini" \
  -H "Gemini-API-Key: SUA_CHAVE_API" \
  -H "Model: gemini-2.0-flash" \
  -H "Prompt: Extraia informa√ß√µes" \
  -F "file=@documento.pdf"
```

## ‚úÖ RESULTADO ESPERADO

Ap√≥s aplicar as corre√ß√µes:

1. ‚úÖ Ollama iniciar√° corretamente dentro do container
2. ‚úÖ API responder√° em `http://localhost:11434/api/tags`
3. ‚úÖ Uploads funcionar√£o sem erro 404
4. ‚úÖ Logs mostrar√£o inicializa√ß√£o bem-sucedida do Ollama
5. ‚úÖ Sistema ter√° fallback autom√°tico para Gemini se necess√°rio

## üîÑ MONITORAMENTO CONT√çNUO

Use os scripts de verifica√ß√£o regularmente:
- `check_services.bat` - Verifica√ß√£o di√°ria
- Logs em tempo real: `docker-compose logs -f`
- Health check: `curl http://localhost:8000/health`

---

**Data**: 09/06/2025  
**Vers√£o**: 1.5  
**Status**: ‚úÖ Implementado e testado 