Aja como um arquiteto de software, desenvolvedor fullstack e engenheiro de IA. Sua tarefa Ã© criar um backend escalÃ¡vel, seguro e leve, que atue como um orquestrador para anÃ¡lise de documentos com auxÃ­lio de modelos de linguagem locais (via Ollama) e em nuvem (via Google Gemini API).

ğŸ†• **VERSÃƒO 1.3 - MULTI-PROVIDER AI**
Este sistema agora suporta dois provedores de IA:
- ğŸ  **Ollama (Local)**: Privacidade total, modelos locais, sem custos
- ğŸŒŸ **Google Gemini (Nuvem)**: Modelos avanÃ§ados, alta performance, sempre atualizados

ğŸ“¥ **Requisitos Funcionais**

## **Endpoint Principal: POST /upload**
Recebe um arquivo no body (ex: arquivo.pdf, arquivo.jpg, arquivo.png, arquivo.docx, arquivo.xlsx, etc.).

### **Headers ObrigatÃ³rios:**
| Header | DescriÃ§Ã£o | Exemplo |
|--------|-----------|---------|
| `Key` | Chave de autenticaÃ§Ã£o para uso da API. Deve ser validada com valor vindo do .env | `myelin-ocr-llm-2024-super-secret-key` |
| `Prompt` | Pergunta sobre o documento | `"Verifique qual CNPJ existe nesse documento"` |
| `Format-Response` | Formato JSON esperado da resposta | `[{"CNPJ": ""}]` |
| `Model` | Nome do modelo a utilizar | `gemma3:1b` ou `gemini-2.0-flash` |
| `AI-Provider` | **ğŸ†• NOVO** - Provedor de IA | `ollama` ou `gemini` |

### **Headers Opcionais:**
| Header | DescriÃ§Ã£o | Exemplo | Quando Usar |
|--------|-----------|---------|-------------|
| `Example` | Exemplo de resposta esperada | `[{"CNPJ": "XX.XXX.XXX/0001-XX"}]` | Para melhor formataÃ§Ã£o |
| `Gemini-API-Key` | **ğŸ†• NOVO** - Chave API Google Gemini | `AIzaSyC...` | Quando `AI-Provider=gemini` |

ğŸ› ï¸ **Requisitos TÃ©cnicos**

## **Infraestrutura Multi-Provider**
- **Container Docker** baseado em Ubuntu LTS
- **Ollama local** com modelos disponÃ­veis (gemma3:1b, llama3:8b, qwen2:0.5b, etc.)
- **IntegraÃ§Ã£o Google Gemini** com biblioteca `google-genai`
- **OCR Tesseract** para imagens
- **Parsers** para PDF, DOC/DOCX, Excel

## **Sistema de Filas Inteligente**
1. **Fila de ExtraÃ§Ã£o**: OCR ou parsing de documentos
2. **Fila de Processamento**: 
   - Rota Ollama (local) OU
   - Rota Gemini (nuvem)
3. **Fila de FormataÃ§Ã£o**: EstruturaÃ§Ã£o da resposta JSON

## **Gerenciamento DinÃ¢mico de Modelos**
### **ğŸ  Ollama (Local)**
- Download automÃ¡tico de modelos via endpoint
- ConfiguraÃ§Ã£o CPU/GPU
- Lista de modelos instalados

### **ğŸŒŸ Gemini (Nuvem)**
- **Lista dinÃ¢mica** de modelos direto da API Google
- Sempre atualizado com novos lanÃ§amentos
- Fallback para modelos conhecidos em caso de erro
- Suporte a modelos avanÃ§ados (gemini-2.0-flash, gemini-2.5-pro-preview)

## **Escalabilidade HÃ­brida**
O sistema deve ser capaz de:
- **Modo Local**: Rodar em mÃ¡quinas modestas usando apenas Ollama
- **Modo HÃ­brido**: Combinar Ollama + Gemini conforme necessidade
- **Modo Nuvem**: Usar apenas Gemini para processamento rÃ¡pido
- **Auto-scaling**: Detectar hardware e configurar workers apropriados

## **Banco de Dados**
- Usar SQLite (foco em leveza)
- Persistir documentos, prompts e respostas
- Campos adicionais: `ai_provider`, `gemini_api_key`, `model_version`
- Limpeza automÃ¡tica configurable (1-24h)

ğŸ“Š **Endpoints Completos**

### **Core Endpoints**
| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `POST` | `/upload` | ğŸš€ Upload inteligente com auto-detecÃ§Ã£o |
| `GET` | `/queue` | Status da fila de processamento |
| `GET` | `/response/:id` | Resposta processada do documento |

### **ğŸ†• Gerenciamento de Modelos**
| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `GET` | `/models/list` | Lista modelos Ollama instalados |
| `POST` | `/models/download` | Download novo modelo Ollama |
| `GET` | `/models/gemini` | **ğŸŒŸ NOVO** - Lista modelos Gemini (dinÃ¢mico) |

### **âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada**
| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `GET` | `/config/compute` | Verificar modo CPU/GPU atual |
| `POST` | `/config/compute` | Configurar CPU/GPU para Ollama |

### **ğŸ” Monitoramento**
| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `GET` | `/health` | Health check da aplicaÃ§Ã£o |
| `GET` | `/` | InformaÃ§Ãµes e documentaÃ§Ã£o da API |

ğŸ“ **OrganizaÃ§Ã£o do Projeto Atualizada**

```
project/
â”œâ”€â”€ ğŸ³ Dockerfile (Ubuntu + Ollama + Python + Tesseract)
â”œâ”€â”€ ğŸš€ docker-compose.yml 
â”œâ”€â”€ ğŸ“‹ requirements.txt (+ google-genai, httpx)
â”œâ”€â”€ ğŸ”§ main.py (FastAPI + endpoints Gemini)
â”œâ”€â”€ ğŸ‘· workers.py (processamento multi-provider)
â”œâ”€â”€ ğŸ› ï¸ utils.py (+ integraÃ§Ã£o Gemini dinÃ¢mica)
â”œâ”€â”€ ğŸ—„ï¸ models.py (+ campos Gemini)
â”œâ”€â”€ ğŸ” .env (+ configuraÃ§Ãµes Gemini)
â”œâ”€â”€ ğŸ“– README.md (documentaÃ§Ã£o completa)
â”œâ”€â”€ ğŸ§ª postman_collection.json (collection v1.3)
â”œâ”€â”€ ğŸ“ Prompt-inicial.txt (este arquivo)
â””â”€â”€ âš™ï¸ start.sh, supervisord.conf
```

ğŸ§ª **Testes Completos**

### **ğŸ  Teste Ollama (Local)**
```bash
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Extraia o CNPJ da empresa" \
  -H "Format-Response: [{\"cnpj\": \"\"}]" \
  -H "Model: gemma3:1b" \
  -H "AI-Provider: ollama" \
  -F "file=@teste.jpg"
```

### **ğŸŒŸ Teste Gemini (Nuvem)**
```bash
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Extraia o CNPJ da empresa" \
  -H "Format-Response: [{\"cnpj\": \"\"}]" \
  -H "Model: gemini-2.0-flash" \
  -H "AI-Provider: gemini" \
  -H "Gemini-API-Key: AIzaSyC..." \
  -F "file=@teste.jpg"
```

### **ğŸ“Š Teste Lista DinÃ¢mica Gemini**
```bash
curl -X GET "http://localhost:8000/models/gemini" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Gemini-API-Key: AIzaSyC..."
```

ğŸ“„ **DocumentaÃ§Ã£o Atualizada**

O README deve incluir:

### **ğŸ¯ SeÃ§Ãµes Principais**
1. **IntroduÃ§Ã£o Multi-Provider**
2. **ConfiguraÃ§Ã£o Ollama vs Gemini**
3. **Como obter chave API Gemini**
4. **Comparativo de Performance**
5. **Exemplos prÃ¡ticos para cada provedor**
6. **Troubleshooting especÃ­fico**

### **ğŸ“Š Tabela Comparativa**
| Aspecto | Ollama | Gemini |
|---------|--------|--------|
| Privacidade | âœ… Total | âš ï¸ Nuvem |
| Custo | âœ… Gratuito | ğŸ’° Por uso |
| Performance | âš¡ VariÃ¡vel | ğŸš€ Consistente |
| Setup | ğŸ”§ Hardware | âœ… API Key |
| Modelos | ğŸ“¦ Download | ğŸ”„ DinÃ¢micos |

ğŸ§  **ConsideraÃ§Ãµes AvanÃ§adas de Escalabilidade**

### **EstratÃ©gias HÃ­bridas**
- **Fallback inteligente**: Ollama â†’ Gemini em caso de sobrecarga
- **Balanceamento por custo**: Tarefas simples â†’ Ollama, complexas â†’ Gemini
- **Roteamento por performance**: Urgentes â†’ Gemini, batch â†’ Ollama

### **OtimizaÃ§Ãµes**
- **Cache de modelos Gemini** para reduzir calls API
- **Pool de conexÃµes** para APIs externas
- **Retry logic** com backoff exponencial
- **Rate limiting** para evitar throttling

ğŸ›¡ï¸ **SeguranÃ§a Aprimorada**

### **ğŸ” AutenticaÃ§Ã£o Multi-Camada**
- **API Key local**: ValidaÃ§Ã£o via .env
- **Chave Gemini**: Tratamento seguro, nÃ£o logada
- **ValidaÃ§Ã£o de provedores**: Verificar AI-Provider vÃ¡lido

### **ğŸ”’ ProteÃ§Ãµes**
- **SanitizaÃ§Ã£o de inputs**: Validar headers e payloads
- **Rate limiting por IP**: Evitar abuso
- **Isolamento de contexto**: Containers separados
- **Logs auditÃ¡veis**: Rastreamento sem dados sensÃ­veis

ğŸ’» **Ambiente e Compatibilidade**

### **Requisitos MÃ­nimos**
- **Para Ollama**: 4GB RAM, CPU 2+ cores
- **Para Gemini**: 1GB RAM, conexÃ£o internet estÃ¡vel
- **Docker**: VersÃ£o 20.10+
- **Sistema**: Windows 11, Linux, macOS

### **ConfiguraÃ§Ãµes Recomendadas**
- **Desenvolvimento**: Ollama local + Gemini para testes
- **ProduÃ§Ã£o leve**: Apenas Gemini
- **ProduÃ§Ã£o completa**: HÃ­brido com balanceamento
- **Enterprise**: Multi-region com cache distribuÃ­do

ğŸš€ **Recursos AvanÃ§ados a Implementar**

### **ğŸŒŸ Funcionalidades Premium**
1. **Auto-seleÃ§Ã£o de modelo** baseada no tipo de documento
2. **AnÃ¡lise de custo em tempo real** (Ollama vs Gemini)
3. **Cache inteligente** de respostas similares
4. **MÃ©tricas de performance** por provedor
5. **Dashboard de monitoramento** em tempo real
6. **API de estatÃ­sticas** de uso e custos

### **ğŸ”§ IntegraÃ§Ãµes Futuras**
- **Anthropic Claude**: Terceiro provedor de IA
- **OpenAI GPT**: IntegraÃ§Ã£o opcional
- **Azure OpenAI**: Para ambientes corporativos
- **AWS Bedrock**: Para infraestrutura na nuvem

Este prompt deve gerar um sistema completo, robusto e futuro-prÃ¡tico, capaz de evoluir com as necessidades do usuÃ¡rio e as inovaÃ§Ãµes em IA.

**ğŸ¯ Meta Final**: Sistema de anÃ¡lise de documentos que combina o melhor dos dois mundos - privacidade e controle local (Ollama) + poder e conveniÃªncia da nuvem (Gemini). 