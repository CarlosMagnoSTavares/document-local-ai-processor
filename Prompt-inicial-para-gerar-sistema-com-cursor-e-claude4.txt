Aja como um arquiteto de software, desenvolvedor fullstack e engenheiro de IA. Sua tarefa é criar um backend escalável, seguro e leve, que atue como um orquestrador para análise de documentos com auxílio de modelos de linguagem locais (via Ollama) e em nuvem (via Google Gemini API).

🆕 **VERSÃO 1.6 - TESTES ABRANGENTES & CORREÇÕES FINAIS**
Este sistema agora inclui:
- 🧪 **Suite de Testes Completa**: 15 testes automatizados com 100% de sucesso
- 🔧 **Correções HTTP 500**: Resolvidos todos os erros de listagem de modelos
- 📊 **Validação Pydantic**: Schemas corrigidos para compatibilidade total
- 🌟 **46+ Modelos Gemini**: Lista dinâmica sempre atualizada
- 🔧 **Sistema de Debug Avançado**: Endpoints para diagnóstico completo
- 🚨 **Correção de Bug Crítico**: Problema de extração de texto resolvido
- 📊 **Monitoramento Aprimorado**: Logs detalhados e verificações de consistência
- 🛠️ **Script de Correção**: Ferramenta automática para identificar e corrigir problemas

🆕 **VERSÃO 1.3 - MULTI-PROVIDER AI**
Este sistema agora suporta dois provedores de IA:
- 🏠 **Ollama (Local)**: Privacidade total, modelos locais, sem custos
- 🌟 **Google Gemini (Nuvem)**: Modelos avançados, alta performance, sempre atualizados

📥 **Requisitos Funcionais**

## **Endpoint Principal: POST /upload**
Recebe um arquivo no body (ex: arquivo.pdf, arquivo.jpg, arquivo.png, arquivo.docx, arquivo.xlsx, etc.).

### **Headers Obrigatórios:**
| Header | Descrição | Exemplo |
|--------|-----------|---------|
| `Key` | Chave de autenticação para uso da API. Deve ser validada com valor vindo do .env | `myelin-ocr-llm-2024-super-secret-key` |
| `Prompt` | Pergunta sobre o documento | `"Verifique qual CNPJ existe nesse documento"` |
| `Format-Response` | Formato JSON esperado da resposta | `[{"CNPJ": ""}]` |
| `Model` | Nome do modelo a utilizar | `gemma3:1b` ou `gemini-2.0-flash` |
| `AI-Provider` | **🆕 NOVO** - Provedor de IA | `ollama` ou `gemini` |

### **Headers Opcionais:**
| Header | Descrição | Exemplo | Quando Usar |
|--------|-----------|---------|-------------|
| `Example` | Exemplo de resposta esperada | `[{"CNPJ": "XX.XXX.XXX/0001-XX"}]` | Para melhor formatação |
| `Gemini-API-Key` | **🆕 NOVO** - Chave API Google Gemini | `AIzaSyC...` | Quando `AI-Provider=gemini` |

🛠️ **Requisitos Técnicos**

## **Infraestrutura Multi-Provider**
- **Container Docker** baseado em Ubuntu LTS
- **Ollama local** com modelos disponíveis (gemma3:1b, llama3:8b, qwen2:0.5b, etc.)
- **Integração Google Gemini** com biblioteca `google-genai`
- **OCR Tesseract** para imagens
- **Parsers** para PDF, DOC/DOCX, Excel

## **Sistema de Filas Inteligente**
1. **Fila de Extração**: OCR ou parsing de documentos
2. **Fila de Processamento**: 
   - Rota Ollama (local) OU
   - Rota Gemini (nuvem)
3. **Fila de Formatação**: Estruturação da resposta JSON

## **Gerenciamento Dinâmico de Modelos**
### **🏠 Ollama (Local)**
- Download automático de modelos via endpoint
- Configuração CPU/GPU
- Lista de modelos instalados

### **🌟 Gemini (Nuvem)**
- **Lista dinâmica** de modelos direto da API Google
- Sempre atualizado com novos lançamentos
- Fallback para modelos conhecidos em caso de erro
- Suporte a modelos avançados (gemini-2.0-flash, gemini-2.5-pro-preview)

## **Escalabilidade Híbrida**
O sistema deve ser capaz de:
- **Modo Local**: Rodar em máquinas modestas usando apenas Ollama
- **Modo Híbrido**: Combinar Ollama + Gemini conforme necessidade
- **Modo Nuvem**: Usar apenas Gemini para processamento rápido
- **Auto-scaling**: Detectar hardware e configurar workers apropriados

## **Banco de Dados**
- Usar SQLite (foco em leveza)
- Persistir documentos, prompts e respostas
- Campos adicionais: `ai_provider`, `gemini_api_key`, `model_version`
- Limpeza automática configurable (1-24h)

📊 **Endpoints Completos**

### **Core Endpoints**
| Método | Endpoint | Descrição |
|--------|----------|-----------|
| `POST` | `/upload` | 🚀 Upload inteligente com auto-detecção |
| `GET` | `/queue` | Status da fila de processamento |
| `GET` | `/response/:id` | Resposta processada do documento |

### **🆕 Gerenciamento de Modelos**
| Método | Endpoint | Descrição |
|--------|----------|-----------|
| `GET` | `/models/list` | Lista modelos Ollama instalados |
| `POST` | `/models/download` | Download novo modelo Ollama |
| `GET` | `/models/gemini` | **🌟 NOVO** - Lista modelos Gemini (dinâmico) |

### **⚙️ Configuração Avançada**
| Método | Endpoint | Descrição |
|--------|----------|-----------|
| `GET` | `/config/compute` | Verificar modo CPU/GPU atual |
| `POST` | `/config/compute` | Configurar CPU/GPU para Ollama |

### **🔍 Monitoramento**
| Método | Endpoint | Descrição |
|--------|----------|-----------|
| `GET` | `/health` | Health check da aplicação |
| `GET` | `/` | Informações e documentação da API |

### **🆕 Debug & Diagnóstico (v1.4)**
| Método | Endpoint | Descrição |
|--------|----------|-----------|
| `GET` | `/debug/document/:id` | **🔧 NOVO** - Diagnóstico completo de documento |
| `GET` | `/response/:id?debug=1` | **📊 NOVO** - Response com informações de debug |

📁 **Organização do Projeto Atualizada**

```
project/
├── 🐳 Dockerfile (Ubuntu + Ollama + Python + Tesseract)
├── 🚀 docker-compose.yml 
├── 📋 requirements.txt (+ google-genai, httpx)
├── 🔧 main.py (FastAPI + endpoints Gemini + Debug)
├── 👷 workers.py (processamento multi-provider + logs detalhados)
├── 🛠️ utils.py (+ integração Gemini dinâmica + logs OCR)
├── 🗄️ models.py (+ campos Gemini)
├── 🔐 .env (+ configurações Gemini)
├── 📖 README.md (documentação completa + troubleshooting)
├── 🧪 postman_collection.json (collection v1.4)
├── 📝 Prompt-inicial.txt (este arquivo)
├── 🔧 fix_extraction_bug.py (🆕 script de correção automática)
└── ⚙️ start.sh, supervisord.conf
```

🧪 **Testes Completos**

### **🎯 Suite de Testes Abrangentes (NOVO!)**
**Status: ✅ 100% de Sucesso (15/15 testes)**

Execute a suite completa:
```bash
python comprehensive_api_test.py
```

**Cobertura de Testes:**
- ✅ Status & Informações (Health check, API info)
- ✅ Gestão de Modelos (Ollama + Gemini - 46+ modelos)
- ✅ Smart Upload Multi-Provider (Ollama + Gemini)
- ✅ Monitoramento (Queue status com/sem debug)
- ✅ Resultados (Document responses com debug)
- ✅ Diagnósticos (Debug completo de documentos)
- ✅ Configuração (CPU/GPU mode management)
- ✅ Segurança (API key validation, error handling)

### **🏠 Teste Ollama (Local)**
```bash
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Extraia o CNPJ da empresa" \
  -H "Format-Response: [{\"cnpj\": \"\"}]" \
  -H "Model: gemma3:1b" \
  -H "AI-Provider: ollama" \
  -F "file=@teste.jpg"
```

### **🌟 Teste Gemini (Nuvem)**
```bash
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Extraia o CNPJ da empresa" \
  -H "Format-Response: [{\"cnpj\": \"\"}]" \
  -H "Model: gemini-2.0-flash" \
  -H "AI-Provider: gemini" \
  -H "Gemini-API-Key: AIzaSyC..." \
  -F "file=@teste.jpg"
```

### **📊 Teste Lista Dinâmica Gemini**
```bash
curl -X GET "http://localhost:8000/models/gemini" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Gemini-API-Key: AIzaSyC..."
```

📄 **Documentação Atualizada**

O README deve incluir:

### **🎯 Seções Principais**
1. **Introdução Multi-Provider**
2. **Configuração Ollama vs Gemini**
3. **Como obter chave API Gemini**
4. **Comparativo de Performance**
5. **Exemplos práticos para cada provedor**
6. **Troubleshooting específico**

### **📊 Tabela Comparativa**
| Aspecto | Ollama | Gemini |
|---------|--------|--------|
| Privacidade | ✅ Total | ⚠️ Nuvem |
| Custo | ✅ Gratuito | 💰 Por uso |
| Performance | ⚡ Variável | 🚀 Consistente |
| Setup | 🔧 Hardware | ✅ API Key |
| Modelos | 📦 Download | 🔄 Dinâmicos |

🧠 **Considerações Avançadas de Escalabilidade**

### **Estratégias Híbridas**
- **Fallback inteligente**: Ollama → Gemini em caso de sobrecarga
- **Balanceamento por custo**: Tarefas simples → Ollama, complexas → Gemini
- **Roteamento por performance**: Urgentes → Gemini, batch → Ollama

### **Otimizações**
- **Cache de modelos Gemini** para reduzir calls API
- **Pool de conexões** para APIs externas
- **Retry logic** com backoff exponencial
- **Rate limiting** para evitar throttling

🛡️ **Segurança Aprimorada**

### **🔐 Autenticação Multi-Camada**
- **API Key local**: Validação via .env
- **Chave Gemini**: Tratamento seguro, não logada
- **Validação de provedores**: Verificar AI-Provider válido

### **🔒 Proteções**
- **Sanitização de inputs**: Validar headers e payloads
- **Rate limiting por IP**: Evitar abuso
- **Isolamento de contexto**: Containers separados
- **Logs auditáveis**: Rastreamento sem dados sensíveis

💻 **Ambiente e Compatibilidade**

### **Requisitos Mínimos**
- **Para Ollama**: 4GB RAM, CPU 2+ cores
- **Para Gemini**: 1GB RAM, conexão internet estável
- **Docker**: Versão 20.10+
- **Sistema**: Windows 11, Linux, macOS

📚 **Documentação Interativa (Swagger/OpenAPI)**

### **🎯 Implementação Obrigatória**
O sistema DEVE incluir documentação Swagger/OpenAPI completa e interativa:

### **📋 Configuração FastAPI**
```python
app = FastAPI(
    title="Document OCR LLM API",
    description="API para análise de documentos com OCR e LLM (Ollama + Gemini)",
    version="1.3.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)
```

### **🏷️ Organização por Tags**
- **📤 Upload de Documentos**: Endpoints de upload e processamento
- **📊 Monitoramento**: Status de fila e progresso
- **📄 Resultados**: Recuperação de respostas processadas
- **🤖 Gestão de Modelos**: Download e listagem (Ollama + Gemini)
- **⚙️ Configuração**: CPU/GPU e configurações avançadas
- **🏥 Saúde**: Health checks e diagnósticos
- **🏠 Informações**: Documentação e informações da API

### **📖 Modelos Pydantic Obrigatórios**
```python
class UploadResponse(BaseModel):
    status: str = Field(description="Status da operação")
    document_id: int = Field(description="ID único do documento")
    ai_provider: str = Field(description="Provedor utilizado")
    extraction_tool: str = Field(description="Ferramenta de extração")

class DocumentResponse(BaseModel):
    status: str = Field(description="Status do processamento")
    data: dict = Field(description="Dados e resultado")

class ModelsListResponse(BaseModel):
    status: str = Field(description="Status da operação")
    provider: str = Field(description="Provedor (ollama/gemini)")
    models: List[ModelInfo] = Field(description="Lista de modelos")
```

### **🔧 Endpoints Documentados**
Cada endpoint DEVE ter:
- **summary**: Título descritivo
- **description**: Explicação detalhada
- **tags**: Categoria apropriada
- **response_model**: Modelo Pydantic de resposta
- **responses**: Códigos de status e descrições
- **parameters**: Descrição completa de headers e parâmetros

### **🌐 URLs de Acesso**
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **OpenAPI JSON**: http://localhost:8000/openapi.json

### **✅ Benefícios Implementados**
- ✅ **Teste sem Postman**: Interface web completa
- ✅ **Documentação Automática**: Sempre sincronizada
- ✅ **Validação Integrada**: Verificação de parâmetros
- ✅ **Exemplos Práticos**: Modelos de uso
- ✅ **Autenticação Visual**: Interface para API keys
- ✅ **Organização Intuitiva**: Navegação por categorias

### **🎯 Requisito de Entrega**
A documentação Swagger DEVE estar 100% funcional e permitir testar todos os endpoints da API diretamente no navegador, eliminando a necessidade do Postman para testes básicos.

🚀 **Recursos Avançados a Implementar**

### **🌟 Funcionalidades Premium**
1. **Auto-seleção de modelo** baseada no tipo de documento
2. **Análise de custo em tempo real** (Ollama vs Gemini)
3. **Cache inteligente** de respostas similares
4. **Métricas de performance** por provedor
5. **Dashboard de monitoramento** em tempo real
6. **API de estatísticas** de uso e custos

### **🔧 Integrações Futuras**
- **Anthropic Claude**: Terceiro provedor de IA
- **OpenAI GPT**: Integração opcional
- **Azure OpenAI**: Para ambientes corporativos
- **AWS Bedrock**: Para infraestrutura na nuvem

Este prompt deve gerar um sistema completo, robusto e futuro-prático, capaz de evoluir com as necessidades do usuário e as inovações em IA.

**🎯 Meta Final**: Sistema de análise de documentos que combina o melhor dos dois mundos - privacidade e controle local (Ollama) + poder e conveniência da nuvem (Gemini).

### **🔧 Endpoints Obrigatórios**

#### **📤 Upload e Processamento**
- `POST /upload` - Upload de documentos com processamento automático
- `GET /response/{document_id}` - Recuperar resultado processado
- `GET /response/{document_id}` + `debug: 1` - **MODO DEBUG AVANÇADO**

#### **🐛 Funcionalidade Debug Obrigatória**
O sistema DEVE implementar modo debug completo no endpoint `/response/{document_id}`:

**Ativação**: Header `debug: 1`

**Retorno Debug (3 seções obrigatórias)**:
1. **Conteúdo Extraído**: Texto OCR/Parser + ferramenta utilizada + estatísticas
2. **Prompt Completo**: Prompt original + prompt construído + configurações + contexto
3. **Resposta Raw LLM**: Resposta bruta + resposta formatada + comparação

**Finalidade**: Troubleshooting de qualidade (OCR vs Prompt vs Modelo)

#### **📊 Monitoramento e Gestão**
```

## 🚨 PROBLEMAS RESOLVIDOS (CRÍTICOS)

### 1. 🤖 ERRO 404 OLLAMA (NOVA CORREÇÃO v1.5)
- **Problema**: Ollama retorna erro 404 em `/api/generate`  
- **Causa**: Ollama não está iniciando corretamente no container
- **Correção**: Implementado sistema completo de verificação e restart
- **Scripts criados**:
  - `check_services.sh` - Verificação rápida de todos os serviços
  - `rebuild_and_debug.sh` - Rebuild completo com debug detalhado
- **Melhorias**: Supervisor com prioridades, timeouts aumentados, variáveis de ambiente corrigidas

### 2. 🐛 TEXTO NÃO EXTRAÍDO (CORRIGIDO v1.4)
