Aja como um arquiteto de software, desenvolvedor fullstack e engenheiro de IA. Sua tarefa Ã© criar um backend escalÃ¡vel, seguro e leve, que atue como um orquestrador para anÃ¡lise de documentos com auxÃ­lio de modelos de linguagem locais (via Ollama) e em nuvem (via Google Gemini API).

ğŸ†• **VERSÃƒO 1.6 - TESTES ABRANGENTES & CORREÃ‡Ã•ES FINAIS**
Este sistema agora inclui:
- ğŸ§ª **Suite de Testes Completa**: 15 testes automatizados com 100% de sucesso
- ğŸ”§ **CorreÃ§Ãµes HTTP 500**: Resolvidos todos os erros de listagem de modelos
- ğŸ“Š **ValidaÃ§Ã£o Pydantic**: Schemas corrigidos para compatibilidade total
- ğŸŒŸ **46+ Modelos Gemini**: Lista dinÃ¢mica sempre atualizada
- ğŸ”§ **Sistema de Debug AvanÃ§ado**: Endpoints para diagnÃ³stico completo
- ğŸš¨ **CorreÃ§Ã£o de Bug CrÃ­tico**: Problema de extraÃ§Ã£o de texto resolvido
- ğŸ“Š **Monitoramento Aprimorado**: Logs detalhados e verificaÃ§Ãµes de consistÃªncia
- ğŸ› ï¸ **Script de CorreÃ§Ã£o**: Ferramenta automÃ¡tica para identificar e corrigir problemas

ğŸ†• **VERSÃƒO 1.3 - MULTI-PROVIDER AI**
Este sistema agora suporta dois provedores de IA:
- ğŸ  **Ollama (Local)**: Privacidade total, modelos locais, sem custos
- ğŸŒŸ **Google Gemini (Nuvem)**: Modelos avanÃ§ados, alta performance, sempre atualizados

ğŸ“¥ **Requisitos Funcionais**

## **Endpoint Principal: POST /upload**
Recebe um arquivo no body (ex: arquivo.pdf, arquivo.jpg, arquivo.png, arquivo.docx, arquivo.xlsx, etc.).

### **Headers ObrigatÃ³rios:**
| Header | DescriÃ§Ã£o | Exemplo |
|--------|-----------|---------|
| `Key` | Chave de autenticaÃ§Ã£o para uso da API. Deve ser validada com valor vindo do .env | `myelin-ocr-llm-2024-super-secret-key` |
| `Prompt` | Pergunta sobre o documento | `"Verifique qual CNPJ existe nesse documento"` |
| `Format-Response` | Formato JSON esperado da resposta | `[{"CNPJ": ""}]` |
| `Model` | Nome do modelo a utilizar | `gemma3:1b` ou `gemini-2.0-flash` |
| `AI-Provider` | **ğŸ†• NOVO** - Provedor de IA | `ollama` ou `gemini` |

### **Headers Opcionais:**
| Header | DescriÃ§Ã£o | Exemplo | Quando Usar |
|--------|-----------|---------|-------------|
| `Example` | Exemplo de resposta esperada | `[{"CNPJ": "XX.XXX.XXX/0001-XX"}]` | Para melhor formataÃ§Ã£o |
| `Gemini-API-Key` | **ğŸ†• NOVO** - Chave API Google Gemini | `AIzaSyC...` | Quando `AI-Provider=gemini` |

ğŸ› ï¸ **Requisitos TÃ©cnicos**

## **Infraestrutura Multi-Provider**
- **Container Docker** baseado em Ubuntu LTS
- **Ollama local** com modelos disponÃ­veis (gemma3:1b, llama3:8b, qwen2:0.5b, etc.)
- **IntegraÃ§Ã£o Google Gemini** com biblioteca `google-genai`
- **OCR Tesseract** para imagens
- **Parsers** para PDF, DOC/DOCX, Excel

## **Sistema de Filas Inteligente**
1. **Fila de ExtraÃ§Ã£o**: OCR ou parsing de documentos
2. **Fila de Processamento**: 
   - Rota Ollama (local) OU
   - Rota Gemini (nuvem)
3. **Fila de FormataÃ§Ã£o**: EstruturaÃ§Ã£o da resposta JSON

## **Gerenciamento DinÃ¢mico de Modelos**
### **ğŸ  Ollama (Local)**
- Download automÃ¡tico de modelos via endpoint
- ConfiguraÃ§Ã£o CPU/GPU
- Lista de modelos instalados

### **ğŸŒŸ Gemini (Nuvem)**
- **Lista dinÃ¢mica** de modelos direto da API Google
- Sempre atualizado com novos lanÃ§amentos
- Fallback para modelos conhecidos em caso de erro
- Suporte a modelos avanÃ§ados (gemini-2.0-flash, gemini-2.5-pro-preview)

## **Escalabilidade HÃ­brida**
O sistema deve ser capaz de:
- **Modo Local**: Rodar em mÃ¡quinas modestas usando apenas Ollama
- **Modo HÃ­brido**: Combinar Ollama + Gemini conforme necessidade
- **Modo Nuvem**: Usar apenas Gemini para processamento rÃ¡pido
- **Auto-scaling**: Detectar hardware e configurar workers apropriados

## **Banco de Dados**
- Usar SQLite (foco em leveza)
- Persistir documentos, prompts e respostas
- Campos adicionais: `ai_provider`, `gemini_api_key`, `model_version`
- Limpeza automÃ¡tica configurable (1-24h)

ğŸ“Š **Endpoints Completos**

### **Core Endpoints**
| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `POST` | `/upload` | ğŸš€ Upload inteligente com auto-detecÃ§Ã£o |
| `GET` | `/queue` | Status da fila de processamento |
| `GET` | `/response/:id` | Resposta processada do documento |

### **ğŸ†• Gerenciamento de Modelos**
| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `GET` | `/models/list` | Lista modelos Ollama instalados |
| `POST` | `/models/download` | Download novo modelo Ollama |
| `GET` | `/models/gemini` | **ğŸŒŸ NOVO** - Lista modelos Gemini (dinÃ¢mico) |

### **âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada**
| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `GET` | `/config/compute` | Verificar modo CPU/GPU atual |
| `POST` | `/config/compute` | Configurar CPU/GPU para Ollama |

### **ğŸ” Monitoramento**
| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `GET` | `/health` | Health check da aplicaÃ§Ã£o |
| `GET` | `/` | InformaÃ§Ãµes e documentaÃ§Ã£o da API |

### **ğŸ†• Debug & DiagnÃ³stico (v1.4)**
| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `GET` | `/debug/document/:id` | **ğŸ”§ NOVO** - DiagnÃ³stico completo de documento |
| `GET` | `/response/:id?debug=1` | **ğŸ“Š NOVO** - Response com informaÃ§Ãµes de debug |

ğŸ“ **OrganizaÃ§Ã£o do Projeto Atualizada**

```
project/
â”œâ”€â”€ ğŸ³ Dockerfile (Ubuntu + Ollama + Python + Tesseract)
â”œâ”€â”€ ğŸš€ docker-compose.yml 
â”œâ”€â”€ ğŸ“‹ requirements.txt (+ google-genai, httpx)
â”œâ”€â”€ ğŸ”§ main.py (FastAPI + endpoints Gemini + Debug)
â”œâ”€â”€ ğŸ‘· workers.py (processamento multi-provider + logs detalhados)
â”œâ”€â”€ ğŸ› ï¸ utils.py (+ integraÃ§Ã£o Gemini dinÃ¢mica + logs OCR)
â”œâ”€â”€ ğŸ—„ï¸ models.py (+ campos Gemini)
â”œâ”€â”€ ğŸ” .env (+ configuraÃ§Ãµes Gemini)
â”œâ”€â”€ ğŸ“– README.md (documentaÃ§Ã£o completa + troubleshooting)
â”œâ”€â”€ ğŸ§ª postman_collection.json (collection v1.4)
â”œâ”€â”€ ğŸ“ Prompt-inicial.txt (este arquivo)
â”œâ”€â”€ ğŸ”§ fix_extraction_bug.py (ğŸ†• script de correÃ§Ã£o automÃ¡tica)
â””â”€â”€ âš™ï¸ start.sh, supervisord.conf
```

ğŸ§ª **Testes Completos**

### **ğŸ¯ Suite de Testes Abrangentes (NOVO!)**
**Status: âœ… 100% de Sucesso (15/15 testes)**

Execute a suite completa:
```bash
python comprehensive_api_test.py
```

**Cobertura de Testes:**
- âœ… Status & InformaÃ§Ãµes (Health check, API info)
- âœ… GestÃ£o de Modelos (Ollama + Gemini - 46+ modelos)
- âœ… Smart Upload Multi-Provider (Ollama + Gemini)
- âœ… Monitoramento (Queue status com/sem debug)
- âœ… Resultados (Document responses com debug)
- âœ… DiagnÃ³sticos (Debug completo de documentos)
- âœ… ConfiguraÃ§Ã£o (CPU/GPU mode management)
- âœ… SeguranÃ§a (API key validation, error handling)

### **ğŸ  Teste Ollama (Local)**
```bash
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Extraia o CNPJ da empresa" \
  -H "Format-Response: [{\"cnpj\": \"\"}]" \
  -H "Model: gemma3:1b" \
  -H "AI-Provider: ollama" \
  -F "file=@teste.jpg"
```

### **ğŸŒŸ Teste Gemini (Nuvem)**
```bash
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Extraia o CNPJ da empresa" \
  -H "Format-Response: [{\"cnpj\": \"\"}]" \
  -H "Model: gemini-2.0-flash" \
  -H "AI-Provider: gemini" \
  -H "Gemini-API-Key: AIzaSyC..." \
  -F "file=@teste.jpg"
```

### **ğŸ“Š Teste Lista DinÃ¢mica Gemini**
```bash
curl -X GET "http://localhost:8000/models/gemini" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Gemini-API-Key: AIzaSyC..."
```

ğŸ“„ **DocumentaÃ§Ã£o Atualizada**

O README deve incluir:

### **ğŸ¯ SeÃ§Ãµes Principais**
1. **IntroduÃ§Ã£o Multi-Provider**
2. **ConfiguraÃ§Ã£o Ollama vs Gemini**
3. **Como obter chave API Gemini**
4. **Comparativo de Performance**
5. **Exemplos prÃ¡ticos para cada provedor**
6. **Troubleshooting especÃ­fico**

### **ğŸ“Š Tabela Comparativa**
| Aspecto | Ollama | Gemini |
|---------|--------|--------|
| Privacidade | âœ… Total | âš ï¸ Nuvem |
| Custo | âœ… Gratuito | ğŸ’° Por uso |
| Performance | âš¡ VariÃ¡vel | ğŸš€ Consistente |
| Setup | ğŸ”§ Hardware | âœ… API Key |
| Modelos | ğŸ“¦ Download | ğŸ”„ DinÃ¢micos |

ğŸ§  **ConsideraÃ§Ãµes AvanÃ§adas de Escalabilidade**

### **EstratÃ©gias HÃ­bridas**
- **Fallback inteligente**: Ollama â†’ Gemini em caso de sobrecarga
- **Balanceamento por custo**: Tarefas simples â†’ Ollama, complexas â†’ Gemini
- **Roteamento por performance**: Urgentes â†’ Gemini, batch â†’ Ollama

### **OtimizaÃ§Ãµes**
- **Cache de modelos Gemini** para reduzir calls API
- **Pool de conexÃµes** para APIs externas
- **Retry logic** com backoff exponencial
- **Rate limiting** para evitar throttling

ğŸ›¡ï¸ **SeguranÃ§a Aprimorada**

### **ğŸ” AutenticaÃ§Ã£o Multi-Camada**
- **API Key local**: ValidaÃ§Ã£o via .env
- **Chave Gemini**: Tratamento seguro, nÃ£o logada
- **ValidaÃ§Ã£o de provedores**: Verificar AI-Provider vÃ¡lido

### **ğŸ”’ ProteÃ§Ãµes**
- **SanitizaÃ§Ã£o de inputs**: Validar headers e payloads
- **Rate limiting por IP**: Evitar abuso
- **Isolamento de contexto**: Containers separados
- **Logs auditÃ¡veis**: Rastreamento sem dados sensÃ­veis

ğŸ’» **Ambiente e Compatibilidade**

### **Requisitos MÃ­nimos**
- **Para Ollama**: 4GB RAM, CPU 2+ cores
- **Para Gemini**: 1GB RAM, conexÃ£o internet estÃ¡vel
- **Docker**: VersÃ£o 20.10+
- **Sistema**: Windows 11, Linux, macOS

ğŸ“š **DocumentaÃ§Ã£o Interativa (Swagger/OpenAPI)**

### **ğŸ¯ ImplementaÃ§Ã£o ObrigatÃ³ria**
O sistema DEVE incluir documentaÃ§Ã£o Swagger/OpenAPI completa e interativa:

### **ğŸ“‹ ConfiguraÃ§Ã£o FastAPI**
```python
app = FastAPI(
    title="Document OCR LLM API",
    description="API para anÃ¡lise de documentos com OCR e LLM (Ollama + Gemini)",
    version="1.3.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)
```

### **ğŸ·ï¸ OrganizaÃ§Ã£o por Tags**
- **ğŸ“¤ Upload de Documentos**: Endpoints de upload e processamento
- **ğŸ“Š Monitoramento**: Status de fila e progresso
- **ğŸ“„ Resultados**: RecuperaÃ§Ã£o de respostas processadas
- **ğŸ¤– GestÃ£o de Modelos**: Download e listagem (Ollama + Gemini)
- **âš™ï¸ ConfiguraÃ§Ã£o**: CPU/GPU e configuraÃ§Ãµes avanÃ§adas
- **ğŸ¥ SaÃºde**: Health checks e diagnÃ³sticos
- **ğŸ  InformaÃ§Ãµes**: DocumentaÃ§Ã£o e informaÃ§Ãµes da API

### **ğŸ“– Modelos Pydantic ObrigatÃ³rios**
```python
class UploadResponse(BaseModel):
    status: str = Field(description="Status da operaÃ§Ã£o")
    document_id: int = Field(description="ID Ãºnico do documento")
    ai_provider: str = Field(description="Provedor utilizado")
    extraction_tool: str = Field(description="Ferramenta de extraÃ§Ã£o")

class DocumentResponse(BaseModel):
    status: str = Field(description="Status do processamento")
    data: dict = Field(description="Dados e resultado")

class ModelsListResponse(BaseModel):
    status: str = Field(description="Status da operaÃ§Ã£o")
    provider: str = Field(description="Provedor (ollama/gemini)")
    models: List[ModelInfo] = Field(description="Lista de modelos")
```

### **ğŸ”§ Endpoints Documentados**
Cada endpoint DEVE ter:
- **summary**: TÃ­tulo descritivo
- **description**: ExplicaÃ§Ã£o detalhada
- **tags**: Categoria apropriada
- **response_model**: Modelo Pydantic de resposta
- **responses**: CÃ³digos de status e descriÃ§Ãµes
- **parameters**: DescriÃ§Ã£o completa de headers e parÃ¢metros

### **ğŸŒ URLs de Acesso**
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **OpenAPI JSON**: http://localhost:8000/openapi.json

### **âœ… BenefÃ­cios Implementados**
- âœ… **Teste sem Postman**: Interface web completa
- âœ… **DocumentaÃ§Ã£o AutomÃ¡tica**: Sempre sincronizada
- âœ… **ValidaÃ§Ã£o Integrada**: VerificaÃ§Ã£o de parÃ¢metros
- âœ… **Exemplos PrÃ¡ticos**: Modelos de uso
- âœ… **AutenticaÃ§Ã£o Visual**: Interface para API keys
- âœ… **OrganizaÃ§Ã£o Intuitiva**: NavegaÃ§Ã£o por categorias

### **ğŸ¯ Requisito de Entrega**
A documentaÃ§Ã£o Swagger DEVE estar 100% funcional e permitir testar todos os endpoints da API diretamente no navegador, eliminando a necessidade do Postman para testes bÃ¡sicos.

ğŸš€ **Recursos AvanÃ§ados a Implementar**

### **ğŸŒŸ Funcionalidades Premium**
1. **Auto-seleÃ§Ã£o de modelo** baseada no tipo de documento
2. **AnÃ¡lise de custo em tempo real** (Ollama vs Gemini)
3. **Cache inteligente** de respostas similares
4. **MÃ©tricas de performance** por provedor
5. **Dashboard de monitoramento** em tempo real
6. **API de estatÃ­sticas** de uso e custos

### **ğŸ”§ IntegraÃ§Ãµes Futuras**
- **Anthropic Claude**: Terceiro provedor de IA
- **OpenAI GPT**: IntegraÃ§Ã£o opcional
- **Azure OpenAI**: Para ambientes corporativos
- **AWS Bedrock**: Para infraestrutura na nuvem

Este prompt deve gerar um sistema completo, robusto e futuro-prÃ¡tico, capaz de evoluir com as necessidades do usuÃ¡rio e as inovaÃ§Ãµes em IA.

**ğŸ¯ Meta Final**: Sistema de anÃ¡lise de documentos que combina o melhor dos dois mundos - privacidade e controle local (Ollama) + poder e conveniÃªncia da nuvem (Gemini).

### **ğŸ”§ Endpoints ObrigatÃ³rios**

#### **ğŸ“¤ Upload e Processamento**
- `POST /upload` - Upload de documentos com processamento automÃ¡tico
- `GET /response/{document_id}` - Recuperar resultado processado
- `GET /response/{document_id}` + `debug: 1` - **MODO DEBUG AVANÃ‡ADO**

#### **ğŸ› Funcionalidade Debug ObrigatÃ³ria**
O sistema DEVE implementar modo debug completo no endpoint `/response/{document_id}`:

**AtivaÃ§Ã£o**: Header `debug: 1`

**Retorno Debug (3 seÃ§Ãµes obrigatÃ³rias)**:
1. **ConteÃºdo ExtraÃ­do**: Texto OCR/Parser + ferramenta utilizada + estatÃ­sticas
2. **Prompt Completo**: Prompt original + prompt construÃ­do + configuraÃ§Ãµes + contexto
3. **Resposta Raw LLM**: Resposta bruta + resposta formatada + comparaÃ§Ã£o

**Finalidade**: Troubleshooting de qualidade (OCR vs Prompt vs Modelo)

#### **ğŸ“Š Monitoramento e GestÃ£o**
```

## ğŸš¨ PROBLEMAS RESOLVIDOS (CRÃTICOS)

### 1. ğŸ¤– ERRO 404 OLLAMA (NOVA CORREÃ‡ÃƒO v1.5)
- **Problema**: Ollama retorna erro 404 em `/api/generate`  
- **Causa**: Ollama nÃ£o estÃ¡ iniciando corretamente no container
- **CorreÃ§Ã£o**: Implementado sistema completo de verificaÃ§Ã£o e restart
- **Scripts criados**:
  - `check_services.sh` - VerificaÃ§Ã£o rÃ¡pida de todos os serviÃ§os
  - `rebuild_and_debug.sh` - Rebuild completo com debug detalhado
- **Melhorias**: Supervisor com prioridades, timeouts aumentados, variÃ¡veis de ambiente corrigidas

### 2. ğŸ› TEXTO NÃƒO EXTRAÃDO (CORRIGIDO v1.4)
