# ğŸ§  Document OCR LLM API

API escalÃ¡vel, segura e leve para anÃ¡lise de documentos com auxÃ­lio de modelos de linguagem locais (Ollama) e em nuvem (Google Gemini).

## ğŸ†• Novidades v1.4 (Testes Abrangentes + CorreÃ§Ãµes)
- âœ… **ğŸ§ª Testes Abrangentes**: Suite completa de testes automatizados com 100% de sucesso
- âœ… **ğŸ”§ CorreÃ§Ãµes HTTP 500**: Resolvidos todos os erros de listagem de modelos
- âœ… **ğŸ“Š ValidaÃ§Ã£o Pydantic**: Schemas corrigidos para compatibilidade total
- âœ… **ğŸŒŸ Google Gemini API**: Suporte completo Ã  API Gemini do Google
- âœ… **ğŸ”€ Multi-Provider**: Alterne entre Ollama (local) e Gemini (nuvem)
- âœ… **ğŸš€ Modelos AvanÃ§ados**: Acesso aos modelos Gemini 2.0/2.5 mais recentes
- âœ… **ğŸ“Š Lista DinÃ¢mica**: Modelos sempre atualizados direto da API Google
- âœ… **ğŸ”‘ SeguranÃ§a**: Suporte a chaves API Gemini seguras
- âœ… **âš¡ Performance**: Modelos em nuvem de alta performance
- âœ… **ğŸ› ï¸ Compatibilidade**: Mesma interface para ambos os provedores

## ğŸ“‹ Ãndice

- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [Arquitetura](#-arquitetura)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [Como Usar](#-como-usar)
- [Endpoints](#-endpoints)
- [Provedores de IA](#-provedores-de-ia)
- [Teste](#-teste)
- [Escalabilidade](#-escalabilidade)
- [SeguranÃ§a](#-seguranÃ§a)
- [Monitoramento](#-monitoramento)
- [Troubleshooting](#-troubleshooting)

## ğŸš€ CaracterÃ­sticas

- **ğŸŒŸ Multi-Provider IA**: Suporte tanto para Ollama (local) quanto Google Gemini (nuvem)
- **ğŸš€ Smart Upload**: DetecÃ§Ã£o automÃ¡tica de tipo de arquivo e ferramenta apropriada
- **âš™ï¸ ConfiguraÃ§Ã£o CPU/GPU**: Controle dinÃ¢mico do modo de processamento Ollama  
- **OCR AvanÃ§ado**: Tesseract para extraÃ§Ã£o de texto de imagens (JPG, PNG)
- **Parsers MÃºltiplos**: PDF, DOCX, XLSX com detecÃ§Ã£o automÃ¡tica
- **LLM Local & Nuvem**: IntegraÃ§Ã£o com Ollama (local) e Google Gemini (nuvem)
- **Gerenciamento de Modelos**: Download e listagem via API (ambos provedores)
- **Sistema de Filas**: Processamento assÃ­ncrono com Celery + Redis
- **Modo Verbose**: Logs detalhados mostrando detecÃ§Ã£o automÃ¡tica
- **Auto-Limpeza**: RemoÃ§Ã£o automÃ¡tica de arquivos e dados antigos
- **Containerizado**: Docker com todas as dependÃªncias
- **EscalÃ¡vel**: Suporte a paralelizaÃ§Ã£o baseada no hardware
- **Monitoramento**: Logs estruturados e health checks avanÃ§ados

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI       â”‚    â”‚   Celery        â”‚    â”‚   Ollama        â”‚
â”‚   (API Server)  â”‚â”€â”€â”€â”€â”‚   (Workers)     â”‚â”€â”€â”€â”€â”‚   (Local LLM)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â–¼
         â”‚                       â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚              â”‚   Google        â”‚
         â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Gemini API    â”‚
         â”‚                                      â”‚   (Cloud LLM)   â”‚
         â–¼                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SQLite        â”‚    â”‚   Redis         â”‚    â”‚   Tesseract     â”‚
â”‚   (Database)    â”‚    â”‚   (Queue)       â”‚    â”‚   (OCR)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Processamento

1. **Upload**: Cliente envia arquivo + headers obrigatÃ³rios (incluindo AI-Provider)
2. **ValidaÃ§Ã£o**: VerificaÃ§Ã£o de API key, tipo e tamanho do arquivo, provider IA
3. **Fila de ExtraÃ§Ã£o**: Worker extrai texto (OCR/Parser)
4. **Fila de LLM**: Worker envia prompt para Ollama (local) ou Gemini (nuvem)
5. **Fila de FormataÃ§Ã£o**: Worker formata resposta final
6. **Resposta**: Cliente consulta resultado via API

## ğŸ› ï¸ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Docker 20.0+
- Docker Compose 2.0+
- 4GB+ RAM (recomendado 8GB+)
- 10GB+ espaÃ§o em disco
- **[OPCIONAL]** Chave API do Google Gemini (para usar modelos em nuvem)

### InstalaÃ§Ã£o via Docker

1. **Clone o repositÃ³rio**
```bash
git clone <repository-url>
cd MYELIN-OCR-LLM-LOCAL
```

2. **Configure o ambiente**
```bash
cp .env.example .env
# Edite o .env com suas configuraÃ§Ãµes
```

3. **Construa e execute o container**
```bash
# OpÃ§Ã£o 1: Docker Compose (recomendado)
docker-compose up --build

# OpÃ§Ã£o 2: Docker direto
docker build -t document-ocr-api .
docker run -p 8000:8000 -p 11434:11434 -v $(pwd)/uploads:/app/uploads document-ocr-api
```

4. **Aguarde a inicializaÃ§Ã£o**
```bash
# Verificar se estÃ¡ funcionando
curl http://localhost:8000/health
```

**Tempo de inicializaÃ§Ã£o**: ~5-10 minutos (download do modelo gemma3:1b)

## ğŸ¤– Provedores de IA

### ğŸ  Ollama (Local)
- **Vantagens**: Privacidade total, sem custos adicionais, funciona offline
- **Modelos**: gemma3:1b, qwen2:0.5b, llama3:8b, mistral:7b, etc.
- **Requisitos**: Hardware local com GPU/CPU adequados

### ğŸŒŸ Google Gemini (Nuvem)
- **Vantagens**: Modelos mais avanÃ§ados, sem requisitos de hardware
- **Modelos**: gemini-2.0-flash, gemini-2.5-pro-preview, gemini-1.5-pro, etc.
- **Requisitos**: Chave API Gemini (gratuita com limites)

### Como Obter Chave API Gemini

1. Acesse [Google AI Studio](https://aistudio.google.com/app/apikey)
2. FaÃ§a login com sua conta Google
3. Clique em "Get API Key"
4. Copie sua chave API
5. Use no header `Gemini-API-Key` das requisiÃ§Ãµes

## âš™ï¸ ConfiguraÃ§Ã£o

### Arquivo .env

```bash
# API Configuration
API_KEY=myelin-ocr-llm-2024-super-secret-key
DEBUG=False
HOST=0.0.0.0
PORT=8000

# Database Configuration
DATABASE_URL=sqlite:///./documents.db

# Redis Configuration
REDIS_URL=redis://localhost:6379/0

# Ollama Configuration (para uso local)
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=gemma3:1b

# File Upload Configuration
MAX_FILE_SIZE=50
ALLOWED_EXTENSIONS=pdf,jpg,jpeg,png,docx,xlsx,xls,doc

# Queue Configuration
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# Cleanup Configuration
CLEANUP_INTERVAL_HOURS=1
MAX_RECORD_AGE_HOURS=24

# Logging Configuration
LOG_LEVEL=INFO
LOG_ROTATION=10MB
```

## ğŸ“– Como Usar

### 1. ğŸ  Usando Ollama (Local)

```bash
# Processamento local com Ollama
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Verifique qual CNPJ existe nesse documento" \
  -H "Format-Response: [{\"CNPJ\": \"\"}]" \
  -H "Model: gemma3:1b" \
  -H "AI-Provider: ollama" \
  -F "file=@documento.pdf"
```

### 2. ğŸŒŸ Usando Google Gemini (Nuvem)

```bash
# Processamento em nuvem com Gemini
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Verifique qual CNPJ existe nesse documento e extraia todas as informaÃ§Ãµes da empresa" \
  -H "Format-Response: [{\"Dia da Leitura\": \"\"}]" \
  -H "Model: gemini-2.0-flash" \
  -H "AI-Provider: gemini" \
  -H "Gemini-API-Key: SUA_CHAVE_API_GEMINI" \
  -H "Example: [{\"Dia da Leitura\": \"31/12/9999\"}]" \
  -F "file=@documento.pdf"
```

**ğŸ› ï¸ DetecÃ§Ã£o AutomÃ¡tica:**
- `.jpg/.png` â†’ Tesseract OCR
- `.pdf` â†’ PyPDF2 Parser  
- `.docx` â†’ python-docx Parser
- `.xlsx` â†’ openpyxl Parser

**Resposta:**
```json
{
  "status": "success",
  "message": "Document uploaded and processing started",
  "document_id": 1,
  "filename": "documento.pdf",
  "ai_provider": "gemini",
  "model": "gemini-2.0-flash"
}
```

### 3. Verificar Status da Fila

```bash
curl -H "Key: myelin-ocr-llm-2024-super-secret-key" "http://localhost:8000/queue"
```

### 4. Obter Resposta

```bash
curl -H "Key: myelin-ocr-llm-2024-super-secret-key" "http://localhost:8000/response/1"
```

### 5. Gerenciamento de Modelos (NOVO!)

#### Listar Modelos DisponÃ­veis
```bash
curl -H "Key: myelin-ocr-llm-2024-super-secret-key" "http://localhost:8000/models/list"
```

**Resposta:**
```json
{
  "status": "success",
  "models": [
    {"name": "gemma3:1b", "status": "available"},
    {"name": "llama3:8b", "status": "available"}
  ],
  "total_models": 2
}
```

#### Baixar Novo Modelo
```bash
curl -X POST "http://localhost:8000/models/download" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Model-Name: llama3:8b"
```

**ğŸ¤– Modelos VÃ¡lidos e Recomendados:**
- **LLaMA 3 (Meta)**: `llama3:8b`, `llama3:70b`
- **Gemma 2 (Google)**: `gemma2:2b`, `gemma2:9b`, `gemma2:27b` 
- **Mistral AI**: `mistral:7b`, `mistral:instruct`
- **Qwen 2 (Alibaba)**: `qwen2:0.5b`, `qwen2:1.5b`, `qwen2:7b`
- **Phi-3 (Microsoft)**: `phi3:mini`, `phi3:medium`

ğŸ’¡ **RecomendaÃ§Ãµes de uso:**
- `qwen2:0.5b` - Mais rÃ¡pido (500MB)
- `gemma2:2b` - Equilibrado (1.6GB)  
- `llama3:8b` - Mais preciso (4.7GB)

âš ï¸ **Importante**: Download pode levar 5-30 minutos dependendo do modelo

### 6. âš™ï¸ ConfiguraÃ§Ã£o CPU/GPU (NOVO!)

#### Verificar Modo Atual
```bash
curl -H "Key: myelin-ocr-llm-2024-super-secret-key" "http://localhost:8000/config/compute"
```

**Resposta:**
```json
{
  "status": "success",
  "compute_mode": "cpu",
  "gpu_enabled": false,
  "cuda_devices": "",
  "config": {
    "OLLAMA_COMPUTE_MODE": "cpu",
    "OLLAMA_GPU_ENABLED": "0",
    "CUDA_VISIBLE_DEVICES": ""
  }
}
```

#### Alternar para GPU (Mais RÃ¡pido)
```bash
curl -X POST "http://localhost:8000/config/compute" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Compute-Mode: gpu"
```

#### Alternar para CPU (EconÃ´mico)
```bash
curl -X POST "http://localhost:8000/config/compute" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Compute-Mode: cpu"
```

**ğŸ”„ ConfiguraÃ§Ã£o Persistente:** As configuraÃ§Ãµes sÃ£o salvas no `.env` e aplicadas automaticamente.

### 7. Usando Modelos Diferentes

```bash
# Upload com LLaMA3:8b (mais preciso)
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: AnÃ¡lise detalhada deste documento" \
  -H "Format-Response: [{\"anÃ¡lise\": \"\", \"detalhes\": \"\"}]" \
  -H "Model: llama3:8b" \
  -F "file=@documento.pdf"
```

## ğŸ”— Endpoints

| MÃ©todo | Endpoint | DescriÃ§Ã£o | Headers ObrigatÃ³rios |
|--------|----------|-----------|---------------------|
| `POST` | `/upload` | ğŸš€ **SMART UPLOAD** - Auto-detecÃ§Ã£o | Key, Prompt, Format-Response, Model, AI-Provider |
| `GET` | `/queue` | Status da fila | Key |
| `GET` | `/response/{id}` | Resposta do documento | Key |
| `POST` | `/models/download` | Download de modelo Ollama | Key, Model-Name |
| `GET` | `/models/list` | Lista modelos Ollama | Key |
| `GET` | `/models/gemini` | **ğŸŒŸ NOVO** - Lista modelos Gemini | Key, Gemini-API-Key |
| `POST` | `/config/compute` | **ğŸ†• NOVO** - Configurar CPU/GPU | Key, Compute-Mode |
| `GET` | `/config/compute` | **ğŸ†• NOVO** - Ver modo atual | Key |
| `GET` | `/health` | Health check | - |
| `GET` | `/` | InformaÃ§Ãµes da API | - |

### Headers ObrigatÃ³rios

| Header | Exemplo | DescriÃ§Ã£o |
|--------|---------|-----------|
| `Key` | `myelin-ocr-llm-2024-super-secret-key` | Chave de autenticaÃ§Ã£o |
| `Prompt` | `"Extraia o CNPJ"` | Pergunta sobre o documento |
| `Format-Response` | `[{"CNPJ": ""}]` | Formato esperado da resposta |
| `Model` | `gemma3:1b` ou `gemini-2.0-flash` | Modelo Ollama ou Gemini a usar |
| `Example` | `[{"CNPJ": "12.345.678/0001-90"}]` | Exemplo de resposta (opcional) |
| `AI-Provider` | `ollama` ou `gemini` | Provider de IA a usar |
| `Gemini-API-Key` | `AIzaSy...` | Chave API Gemini (obrigatÃ³ria quando AI-Provider=gemini) |

### ğŸŒŸ Exemplos de Uso Gemini

#### Listar Modelos Gemini DisponÃ­veis
```bash
curl -X GET "http://localhost:8000/models/gemini" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Gemini-API-Key: SUA_CHAVE_API_GEMINI"
```

**Resposta:**
```json
{
  "status": "success",
  "provider": "gemini",
  "models": [
    {
      "name": "gemini-2.0-flash",
      "display_name": "Gemini 2.0 Flash",
      "description": "Latest multimodal model with next generation features",
      "input_token_limit": "1,048,576",
      "output_token_limit": "8,192"
    },
    {
      "name": "gemini-2.5-pro-preview",
      "display_name": "Gemini 2.5 Pro Preview", 
      "description": "Most powerful thinking model with maximum accuracy",
      "input_token_limit": "1,048,576",
      "output_token_limit": "65,536"
    }
  ],
  "total_models": 7,
  "documentation": "https://ai.google.dev/gemini-api/docs/models"
}
```

#### ComparaÃ§Ã£o de Modelos Recomendados

| Caso de Uso | Ollama (Local) | Gemini (Nuvem) | Vantagem |
|-------------|----------------|-----------------|----------|
| **Documentos Simples** | gemma3:1b | gemini-2.0-flash-lite | Local: Privacidade |
| **AnÃ¡lise Complexa** | llama3:8b | gemini-2.0-flash | Nuvem: Performance |
| **RaciocÃ­nio AvanÃ§ado** | llama3:70b | gemini-2.5-pro-preview | Nuvem: Capacidade |
| **Alto Volume** | qwen2:0.5b | gemini-1.5-flash-8b | Local: Sem limite de requests |
| **Processamento Offline** | Qualquer | âŒ NÃ£o disponÃ­vel | Local: Funcionamento offline |

### Status de Processamento

- `uploaded`: Arquivo recebido
- `text_extracted`: Texto extraÃ­do com sucesso
- `prompt_processed`: LLM processou o prompt

## ğŸ“š DocumentaÃ§Ã£o Interativa (Swagger)

A API possui documentaÃ§Ã£o interativa completa via Swagger/OpenAPI, permitindo testar todos os endpoints diretamente no navegador sem necessidade do Postman.

### ğŸŒ Acessando a DocumentaÃ§Ã£o

Com a aplicaÃ§Ã£o rodando, acesse:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc  
- **OpenAPI JSON**: http://localhost:8000/openapi.json

### ğŸš€ Funcionalidades da DocumentaÃ§Ã£o

- **ğŸ“‹ Endpoints Organizados**: Agrupados por categorias com emojis
  - ğŸ“¤ Upload de Documentos
  - ğŸ“Š Monitoramento  
  - ğŸ“„ Resultados
  - ğŸ¤– GestÃ£o de Modelos
  - âš™ï¸ ConfiguraÃ§Ã£o
  - ğŸ¥ SaÃºde
  - ğŸ  InformaÃ§Ãµes

- **ğŸ”§ Teste Interativo**: Execute requisiÃ§Ãµes diretamente na interface
- **ğŸ“– DocumentaÃ§Ã£o Detalhada**: DescriÃ§Ãµes completas de parÃ¢metros e respostas
- **ğŸ” AutenticaÃ§Ã£o**: Interface para inserir chave API
- **ğŸ“ Exemplos**: Modelos de request/response para cada endpoint
- **ğŸ¯ ValidaÃ§Ã£o**: ValidaÃ§Ã£o automÃ¡tica de parÃ¢metros

### ğŸ’¡ Como Usar o Swagger

1. **Acesse**: http://localhost:8000/docs
2. **Autentique**: Clique em "Authorize" e insira sua API key
3. **Explore**: Navegue pelos endpoints organizados por categoria
4. **Teste**: Clique em "Try it out" para testar qualquer endpoint
5. **Execute**: Preencha os parÃ¢metros e clique em "Execute"

### ğŸ¯ Vantagens do Swagger

- âœ… **Sem Postman**: Teste direto no navegador
- âœ… **DocumentaÃ§Ã£o Sempre Atualizada**: Sincronizada automaticamente com o cÃ³digo
- âœ… **Interface AmigÃ¡vel**: NavegaÃ§Ã£o intuitiva e organizada
- âœ… **ValidaÃ§Ã£o AutomÃ¡tica**: Verifica parÃ¢metros antes do envio
- âœ… **Exemplos PrÃ¡ticos**: Modelos de uso para cada endpoint
- âœ… **Suporte Completo**: Todos os endpoints documentados

### ğŸ“‹ Exemplo de Teste via Swagger

1. Acesse http://localhost:8000/docs
2. Clique em "Authorize" e insira: `myelin-ocr-llm-2024-super-secret-key`
3. Expanda "ğŸ“¤ Upload de Documentos" â†’ "POST /upload"
4. Clique em "Try it out"
5. Preencha os headers:
   - Prompt: `"Extraia o CNPJ deste documento"`
   - Format-Response: `[{"CNPJ": ""}]`
   - Model: `gemma3:1b`
   - AI-Provider: `ollama`
6. FaÃ§a upload de um arquivo
7. Clique em "Execute"
8. Veja a resposta em tempo real!

## ğŸ§ª Teste

### ğŸ¯ Suite de Testes Abrangentes (NOVO!)

**Status Atual: âœ… 100% de Sucesso (15/15 testes passando)**

Execute a suite completa de testes automatizados:

```bash
python comprehensive_api_test.py
```

**Cobertura de Testes:**
- âœ… **Status & InformaÃ§Ãµes**: Health check e informaÃ§Ãµes da API
- âœ… **GestÃ£o de Modelos**: Listagem Ollama e Gemini (46+ modelos)
- âœ… **Smart Upload**: Upload multi-provider (Ollama + Gemini)
- âœ… **Monitoramento**: Status da fila com/sem debug
- âœ… **Resultados**: RecuperaÃ§Ã£o de respostas com debug detalhado
- âœ… **DiagnÃ³sticos**: Debug completo de documentos
- âœ… **ConfiguraÃ§Ã£o**: GestÃ£o de modo CPU/GPU
- âœ… **SeguranÃ§a**: ValidaÃ§Ã£o de chaves API e erros

**Exemplo de SaÃ­da:**
```
ğŸ§  COMPREHENSIVE API TEST SUITE - Document OCR LLM API
================================================================================
ğŸ“Š Total Tests: 15
âœ… Passed: 15
âŒ Failed: 0
ğŸ“ˆ Success Rate: 100.0%
================================================================================
```

### Script de Teste Automatizado

```bash
# Windows
test_api.bat

# Linux/Mac
chmod +x test_api.sh
./test_api.sh
```

### Collection do Postman

1. **Importe os arquivos no Postman:**
   - `postman_collection.json` (Collection)
   - `postman_environment.json` (Environment)

2. **Configure o ambiente** "Document OCR LLM API - Local"

3. **Como usar:**
   - Execute "Health Check" primeiro
   - Execute "Upload Document (Image)" com arquivo `teste.jpg`
   - O `document_id` serÃ¡ preenchido automaticamente
   - Execute "Get Queue Status" para acompanhar
   - Execute "Get Document Response" para o resultado

### Testes Manuais

```bash
# 1. Health Check
curl http://localhost:8000/health

# 2. Upload com teste.jpg
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Descreva o que vocÃª vÃª na imagem" \
  -H "Format-Response: {\"descricao\": \"\"}" \
  -H "Model: gemma3:1b" \
  -H "AI-Provider: ollama" \
  -F "file=@teste.jpg"

# 3. Verificar status
curl -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  "http://localhost:8000/queue"
```

## ğŸ“ˆ Escalabilidade

### ConfiguraÃ§Ã£o para MÃ¡quinas Modestas

```bash
# No .env
CELERY_WORKERS=1
CELERY_CONCURRENCY=1
MODEL=gemma3:1b
```

### ConfiguraÃ§Ã£o para MÃ¡quinas Potentes

```bash
# No .env
CELERY_WORKERS=4
CELERY_CONCURRENCY=4
MODEL=llama2:7b
```

### Escalonamento Manual

```bash
# Adicionar mais workers
docker exec -d <container> celery -A workers worker --concurrency=4

# Monitorar workers
docker exec <container> celery -A workers inspect active
```

## ğŸ”’ SeguranÃ§a

### ValidaÃ§Ãµes Implementadas

- âœ… AutenticaÃ§Ã£o via API Key
- âœ… ValidaÃ§Ã£o de tipos de arquivo
- âœ… LimitaÃ§Ã£o de tamanho de arquivo
- âœ… SanitizaÃ§Ã£o de inputs
- âœ… Rate limiting (via Celery)

### ConfiguraÃ§Ãµes de SeguranÃ§a

```bash
# Tipos de arquivo permitidos
ALLOWED_EXTENSIONS=pdf,jpg,jpeg,png,docx,xlsx

# Tamanho mÃ¡ximo
MAX_FILE_SIZE=50

# API Key forte
API_KEY=myelin-ocr-llm-2024-super-secret-key
```

## ğŸ“Š Monitoramento (Modo Verbose Ativo!)

### Logs Detalhados em Tempo Real

O sistema agora opera em **modo verbose** com logs detalhados de todo o processo:

```bash
# Ver logs em tempo real do container
docker logs <container_name> -f

# Logs especÃ­ficos por serviÃ§o (com modo verbose)
docker exec -it <container_name> tail -f /var/log/supervisor/fastapi.log     # FastAPI verbose
docker exec -it <container_name> tail -f /var/log/supervisor/celery_worker.log  # Celery debug
docker exec -it <container_name> tail -f /var/log/supervisor/ollama.log        # Ollama verbose

# Logs da aplicaÃ§Ã£o (dentro do container)
docker exec -it <container_name> tail -f /app/logs/app.log        # Logs gerais
docker exec -it <container_name> tail -f /app/logs/verbose.log    # Logs verbose especÃ­ficos
```

### ğŸ“‹ O que vocÃª verÃ¡ nos logs verbose:

#### ğŸ“¤ **Smart Upload Process**
```
ğŸ“¤ VERBOSE: Starting document upload process
ğŸ“ VERBOSE: Filename: teste.jpg
ğŸ¤– VERBOSE: Model requested: gemma3:1b
ğŸ“ VERBOSE: File size: 2.45 MB
ğŸ” VERBOSE: Auto-detected file type: JPG
ğŸ› ï¸ VERBOSE: Will use extraction tool: Tesseract OCR
ğŸ’¾ VERBOSE: Saving file to disk...
âœ… VERBOSE: File saved to: uploads/uuid_teste.jpg
ğŸ—„ï¸ VERBOSE: Creating database record...
ğŸš€ VERBOSE: Starting Celery task for document 123
```

#### ğŸ” **OCR Process**
```
ğŸ” VERBOSE: Starting text extraction from JPG file: uploads/uuid_teste.jpg
ğŸ–¼ï¸ VERBOSE: Using OCR (Tesseract) for image processing
âœ… VERBOSE: Text extraction completed. Extracted 847 characters
ğŸ“„ VERBOSE: Text preview: NOTA FISCAL ELETRÃ”NICA...
```

#### ğŸ¤– **LLM Process**
```
ğŸ¤– VERBOSE: Sending prompt to Ollama model 'gemma3:1b'
ğŸ“„ VERBOSE: Context length: 847 characters
â“ VERBOSE: Prompt: Verifique qual CNPJ existe nesse documento
ğŸ”— VERBOSE: Making request to http://localhost:11434/api/generate
âœ… VERBOSE: Ollama response received (234 chars)
ğŸ’¬ VERBOSE: Response preview: O CNPJ encontrado no documento Ã©...
â±ï¸ VERBOSE: Total duration: 3.45s
ğŸ”¢ VERBOSE: Prompt tokens: 298
ğŸ“Š VERBOSE: Response tokens: 87
```

#### âš™ï¸ **CPU/GPU Configuration**
```
ğŸ”„ VERBOSE: Switching Ollama compute mode to: GPU
ğŸš€ VERBOSE: GPU mode enabled - CUDA devices accessible
ğŸ”„ VERBOSE: Restarting Ollama service with GPU mode...
âœ… VERBOSE: Ollama restarted successfully in GPU mode
â„¹ï¸ VERBOSE: Current compute mode: GPU
```

### Logs Tradicionais

### Health Checks

```bash
# API Health
curl http://localhost:8000/health

# Ollama Health
curl http://localhost:11434/api/tags

# Redis Health
docker exec <container> redis-cli ping
```

### MÃ©tricas

```bash
# Status dos workers
docker exec <container> celery -A workers inspect stats

# Fila de tarefas
docker exec <container> celery -A workers inspect reserved

# Uso de recursos
docker stats <container_name>
```

## ğŸ”§ Troubleshooting

### Problemas Comuns

#### 1. ğŸš¨ Status COMPLETED mas "Texto ainda nÃ£o extraÃ­do"

**Sintoma**: Documento aparece como COMPLETED no `/queue` mas retorna "Texto ainda nÃ£o extraÃ­do" no `/response/{id}`

**Causa**: Bug no pipeline de extraÃ§Ã£o onde o texto nÃ£o Ã© salvo corretamente no banco de dados

**SoluÃ§Ãµes**:

1. **DiagnÃ³stico AutomÃ¡tico**:
```bash
# Use o endpoint de debug para diagnÃ³stico completo
curl -X GET "http://localhost:8000/debug/document/1" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key"
```

2. **Script de CorreÃ§Ã£o**:
```bash
# Execute o script de correÃ§Ã£o automÃ¡tica
python fix_extraction_bug.py
```

3. **VerificaÃ§Ã£o Manual**:
```bash
# Verifique logs detalhados com debug=1
curl -X GET "http://localhost:8000/response/1" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "debug: 1"
```

4. **Rebuild Docker** (se necessÃ¡rio):
```bash
docker-compose down
docker-compose build --no-cache
docker-compose up -d
```

#### 2. ğŸ–¼ï¸ OCR nÃ£o funciona (Tesseract)

**Sintomas**: Erro ao processar imagens JPG/PNG

**SoluÃ§Ãµes**:
```bash
# Verificar se Tesseract estÃ¡ instalado no container
docker exec -it <container_name> tesseract --version

# Reinstalar dependÃªncias se necessÃ¡rio
docker-compose build --no-cache
```

#### 3. ğŸ¤– Ollama nÃ£o responde (Erro 404)

**Sintomas**: Erro 404 ao acessar `http://localhost:11434/api/generate`

**Causa**: Ollama nÃ£o estÃ¡ rodando ou nÃ£o estÃ¡ configurado corretamente no container

**SoluÃ§Ãµes**:

1. **VerificaÃ§Ã£o RÃ¡pida**:
```bash
# Usar script de verificaÃ§Ã£o
./check_services.sh

# Ou testar manualmente
curl http://localhost:11434/api/tags
```

2. **Restart Simples**:
```bash
# Reiniciar apenas o container
docker-compose restart

# Aguardar 30-60 segundos e testar novamente
curl http://localhost:11434/api/tags
```

3. **Rebuild Completo**:
```bash
# Usar script automÃ¡tico
./rebuild_and_debug.sh

# Ou manualmente
docker-compose down -v
docker-compose build --no-cache
docker-compose up -d
```

4. **Verificar Logs do Ollama**:
```bash
# Ver logs especÃ­ficos do Ollama
docker-compose logs document-ocr-api | grep -E "(ollama|Ollama)"

# Entrar no container para debug
docker exec -it $(docker-compose ps -q) bash
ollama serve --help
ps aux | grep ollama
```

5. **Usar Gemini como Alternativa**:
```bash
# Se Ollama falhar, use Gemini
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Extraia informaÃ§Ãµes do documento" \
  -H "Format-Response: [{\"info\": \"\"}]" \
  -H "Model: gemini-2.0-flash" \
  -H "AI-Provider: gemini" \
  -H "Gemini-API-Key: SUA_CHAVE_API" \
  -F "file=@documento.pdf"
```

#### 4. ğŸŒŸ Gemini API nÃ£o funciona

**Sintomas**: Erro 401 ou 403 com Gemini

**SoluÃ§Ãµes**:
- Verificar se a chave API estÃ¡ correta
- Verificar se hÃ¡ quota disponÃ­vel
- Testar chave API diretamente:
```bash
curl -X GET "http://localhost:8000/models/gemini" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Gemini-API-Key: SUA_CHAVE_API"
```

#### 5. ğŸ“Š Redis/Celery nÃ£o processa

**Sintomas**: Documentos ficam presos em UPLOADED

**SoluÃ§Ãµes**:
```bash
# Verificar Redis
docker-compose logs redis

# Verificar workers Celery
docker-compose logs worker

# Reiniciar workers
docker-compose restart worker
```

### Logs e Monitoramento

#### Logs Detalhados
```bash
# Ver logs em tempo real
docker-compose logs -f

# Logs especÃ­ficos do worker
docker-compose logs -f worker

# Logs com timestamps
docker-compose logs -t
```

#### Health Checks
```bash
# Verificar saÃºde da aplicaÃ§Ã£o
curl http://localhost:8000/health

# Verificar fila de processamento
curl -X GET "http://localhost:8000/queue" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key"
```

### Endpoints de Debug

#### 1. Debug de Documento EspecÃ­fico
```bash
GET /debug/document/{document_id}
```
Retorna diagnÃ³stico completo incluindo:
- Status do arquivo no sistema
- VerificaÃ§Ã£o de extraÃ§Ã£o de texto
- Status do processamento LLM
- InconsistÃªncias detectadas
- Teste de re-extraÃ§Ã£o

#### 2. Debug de Response
```bash
GET /response/{document_id}
Header: debug=1
```
Retorna informaÃ§Ãµes detalhadas:
- ConteÃºdo extraÃ­do pelo OCR/Parser
- Prompt completo enviado para LLM
- Resposta raw da LLM antes da formataÃ§Ã£o

### Script de CorreÃ§Ã£o AutomÃ¡tica

O arquivo `fix_extraction_bug.py` oferece:

1. **VerificaÃ§Ã£o de DependÃªncias**: Testa se todas as bibliotecas estÃ£o instaladas
2. **DiagnÃ³stico de Banco**: Identifica documentos com problemas
3. **Teste de ExtraÃ§Ã£o**: Testa extraÃ§Ã£o em arquivos reais
4. **CorreÃ§Ã£o AutomÃ¡tica**: Tenta corrigir documentos problemÃ¡ticos

```bash
# Executar diagnÃ³stico completo
python fix_extraction_bug.py

# Ou dentro do container Docker
docker exec -it <container_name> python fix_extraction_bug.py
```

### PrevenÃ§Ã£o de Problemas

1. **Monitoramento Regular**:
   - Use o endpoint `/queue` para verificar documentos presos
   - Configure alertas para documentos em processamento hÃ¡ muito tempo

2. **Backup Regular**:
   - FaÃ§a backup do banco `documents.db`
   - Mantenha logs para anÃ¡lise posterior

3. **AtualizaÃ§Ãµes**:
   - Mantenha Docker e dependÃªncias atualizados
   - Teste em ambiente de desenvolvimento antes de produÃ§Ã£o

4. **Recursos Adequados**:
   - Monitore uso de CPU/RAM
   - Ajuste workers Celery conforme necessÃ¡rio

## ğŸš€ URLs Importantes

- **API Principal**: http://localhost:8000
- **DocumentaÃ§Ã£o**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health
- **Ollama**: http://localhost:11434

## ğŸ“„ Arquivos de ConfiguraÃ§Ã£o

- `build_and_run.bat` - Script de build para Windows
- `test_api.bat` - Script de teste para Windows
- `postman_collection.json` - Collection do Postman
- `postman_environment.json` - Environment do Postman
- `.env` - ConfiguraÃ§Ãµes da aplicaÃ§Ã£o
- `docker-compose.yml` - OrquestraÃ§Ã£o de containers

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **GNU Affero General Public License v3.0 (AGPL-3.0)**.

### ğŸ†“ Uso Livre Permitido
- âœ… **Uso pessoal e educacional**
- âœ… **Pesquisa e desenvolvimento**
- âœ… **Projetos open source** (devem tambÃ©m usar licenÃ§a compatÃ­vel)
- âœ… **ContribuiÃ§Ãµes** para este projeto

### ğŸ¢ Uso Comercial
Para **uso comercial**, incluindo:
- ğŸ’¼ IntegraÃ§Ã£o em produtos comerciais
- ğŸŒ Oferecimento como serviÃ§o (SaaS)
- ğŸ’° Venda ou distribuiÃ§Ã£o comercial
- ğŸ­ Uso em ambiente empresarial com fins lucrativos

**Ã‰ necessÃ¡ria uma licenÃ§a comercial separada.** Entre em contato:

ğŸ“§ **Email**: carlosmagnosilvatavares@gmail.com  
ğŸ’¬ **LinkedIn**: https://www.linkedin.com/in/carlosmagnosilvatavares/
ğŸ“„ **Termos**: NegociÃ¡veis conforme o caso de uso


### âš–ï¸ Compliance AGPL
Ao usar este software sob AGPL-3.0, vocÃª deve:
- ğŸ“– **Manter avisos de copyright** em todos os arquivos
- ğŸ”“ **Disponibilizar cÃ³digo fonte** de qualquer modificaÃ§Ã£o
- ğŸ“‹ **Usar licenÃ§a compatÃ­vel** em projetos derivados
- ğŸŒ **Fornecer cÃ³digo fonte** mesmo para uso como serviÃ§o web

### ğŸ“‹ LicenÃ§as de Terceiros
Este projeto utiliza bibliotecas open source com suas respectivas licenÃ§as:
- **FastAPI**: MIT License
- **Tesseract OCR**: Apache 2.0 License  
- **Ollama**: MIT License
- **PostgreSQL**: PostgreSQL License
- **Redis**: BSD License

Para mais detalhes, consulte o arquivo [LICENSE](LICENSE).

### ğŸ¤ ContribuiÃ§Ãµes
ContribuiÃ§Ãµes sÃ£o bem-vindas! Ao contribuir, vocÃª concorda que suas contribuiÃ§Ãµes serÃ£o licenciadas sob os mesmos termos deste projeto.

Para contribuir:
1. FaÃ§a um fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

---

ğŸ¯ **Pronto para usar!** O sistema estÃ¡ configurado para rodar em mÃ¡quinas modestas e escalar conforme necessÃ¡rio.

ğŸ“œ **Licenciado sob AGPL-3.0** - Uso livre para projetos compatÃ­veis, licenÃ§a comercial para uso empresarial.

---

**ğŸ§  Powered by Myelin AI - Processamento inteligente de documentos**

## ğŸ¯ **Sobre o Desenvolvimento deste Sistema**

### ğŸ“ **Prompt-inicial-para-gerar-sistema-com-cursor-e-claude4.txt**

Este sistema foi **inteiramente gerado** atravÃ©s de engenharia de prompt avanÃ§ada Criado pelo engenheiro de dados Carlos Magno utilizando **Claude Sonnet 4** no **Cursor IDE**. O arquivo `Prompt-inicial-para-gerar-sistema-com-cursor-e-claude4.txt` contÃ©m o prompt original e atualizado que foi usado para criar toda a aplicaÃ§Ã£o.

#### ğŸ”„ **Como Funciona:**

1. **ğŸ“‹ Prompt Inicial**: O arquivo contÃ©m especificaÃ§Ãµes completas do sistema (Feito na MÃ£o por um Humano)
1.2 **ğŸ“‹ Prompt Inicial Melhorado**: Foi feito com base no inicial, porÃ©m melhorado com GPT 4o
1.2.3 **ğŸ“‹ Prompt Inicial Melhorado Tecnicamente**: A parte tÃ©cnica foi melhorada usando Claude 4 Thinking mode
2. **ğŸ¤– Claude Sonnet 4**: Interpreta e gera cÃ³digo baseado nas especificaÃ§Ãµes
3. **âš¡ Cursor IDE**: Facilita a implementaÃ§Ã£o e refinamento do cÃ³digo
4. **ğŸ”§ IteraÃ§Ãµes**: Melhorias contÃ­nuas atravÃ©s de prompts refinados

#### ğŸ—ï¸ **Arquitetura do Prompt:**

```
ğŸ“ Prompt Inicial v1.3
â”œâ”€â”€ ğŸ¯ Objetivo Principal
â”œâ”€â”€ ğŸ“¥ Requisitos Funcionais
â”‚   â”œâ”€â”€ Headers obrigatÃ³rios
â”‚   â”œâ”€â”€ Headers opcionais  
â”‚   â””â”€â”€ Multi-provider support
â”œâ”€â”€ ğŸ› ï¸ Requisitos TÃ©cnicos
â”‚   â”œâ”€â”€ Infraestrutura Docker
â”‚   â”œâ”€â”€ Sistema de filas
â”‚   â”œâ”€â”€ Gerenciamento de modelos
â”‚   â””â”€â”€ Banco de dados
â”œâ”€â”€ ğŸ“Š Endpoints Completos
â”‚   â”œâ”€â”€ Core endpoints
â”‚   â”œâ”€â”€ Gerenciamento de modelos
â”‚   â”œâ”€â”€ ConfiguraÃ§Ã£o avanÃ§ada
â”‚   â””â”€â”€ Monitoramento
â”œâ”€â”€ ğŸ§ª Testes e Exemplos
â”œâ”€â”€ ğŸ“ OrganizaÃ§Ã£o do projeto
â”œâ”€â”€ ğŸ›¡ï¸ SeguranÃ§a
â””â”€â”€ ğŸš€ Recursos avanÃ§ados
```

#### ğŸ“ˆ **EvoluÃ§Ã£o do Sistema:**

| VersÃ£o | Prompt Focus | Resultado |
|--------|--------------|-----------|
| **v1.0** | Sistema bÃ¡sico OCR + Ollama | Core functionality |
| **v1.1** | Smart upload + Auto-detection | File type detection |
| **v1.2** | CPU/GPU config + Model management | Performance optimization |
| **v1.3** | **Multi-provider AI + Gemini** | **Cloud + Local hybrid** |

#### ğŸ¨ **Metodologia de Desenvolvimento:**

1. **ğŸ“‹ EspecificaÃ§Ã£o Detalhada**
   - Requisitos funcionais claros
   - Exemplos de uso concretos
   - Estrutura de arquivos definida

2. **ğŸ¤– GeraÃ§Ã£o Assistida por IA**
   - Claude Sonnet 4 para lÃ³gica complexa
   - Cursor IDE para refinamentos
   - IteraÃ§Ãµes baseadas em feedback

3. **ğŸ”§ Refinamento ContÃ­nuo**
   - Testes em tempo real
   - Melhorias baseadas em uso
   - DocumentaÃ§Ã£o auto-atualizada

4. **ğŸ“Š ValidaÃ§Ã£o PrÃ¡tica**
   - Collection Postman completa
   - Testes automatizados
   - Logs detalhados para debug

#### ğŸ’¡ **Por que este MÃ©todo Ã© Eficaz:**

âœ… **EspecificaÃ§Ã£o Completa**: O prompt detalha cada aspecto do sistema
âœ… **ConsistÃªncia Arquitetural**: MantÃ©m padrÃµes em todo o cÃ³digo
âœ… **DocumentaÃ§Ã£o AutomÃ¡tica**: README e collection gerados automaticamente
âœ… **Testabilidade**: Inclui testes e validaÃ§Ãµes desde o inÃ­cio
âœ… **Escalabilidade**: Arquitetura pensada para crescimento
âœ… **Manutenibilidade**: CÃ³digo limpo e bem estruturado

#### ğŸ”„ **Como Usar o Prompt:**

1. **ğŸ“ Para Recriar o Sistema:**
   ```bash
   # Cole o conteÃºdo do arquivo no Claude Sonnet 4
   # Especifique ajustes desejados
   # Execute no Cursor IDE
   ```

2. **ğŸ”§ Para ModificaÃ§Ãµes:**
   ```bash
   # Edite seÃ§Ãµes especÃ­ficas do prompt
   # Mantenha a estrutura geral
   # Atualize exemplos conforme necessÃ¡rio
   ```

3. **ğŸ“ˆ Para Novas Funcionalidades:**
   ```bash
   # Adicione Ã  seÃ§Ã£o "Recursos AvanÃ§ados"
   # Especifique requisitos tÃ©cnicos
   # Inclua exemplos de teste
   ```

#### ğŸ¯ **BenefÃ­cios da Abordagem:**

| Vantagem | DescriÃ§Ã£o |
|----------|-----------|
| **âš¡ Velocidade** | Sistema completo em horas, nÃ£o semanas |
| **ğŸ“Š Qualidade** | PadrÃµes consistentes e best practices |
| **ğŸ”§ Flexibilidade** | FÃ¡cil modificaÃ§Ã£o via prompt updates |
| **ğŸ“š DocumentaÃ§Ã£o** | Auto-gerada e sempre atualizada |
| **ğŸ§ª Testabilidade** | Testes incluÃ­dos desde o design |
| **ğŸ”„ IteraÃ§Ã£o** | Melhorias rÃ¡pidas baseadas em feedback |

#### ğŸš€ **Futuro do Desenvolvimento:**

Esta metodologia representa o **futuro do desenvolvimento de software**:
- **ğŸ¤– IA como Copiloto AvanÃ§ado**: NÃ£o apenas sugestÃµes, mas arquitetura completa
- **ğŸ“ EspecificaÃ§Ã£o Declarativa**: Descrever "o que" ao invÃ©s de "como"
- **ğŸ”„ IteraÃ§Ã£o RÃ¡pida**: MudanÃ§as arquiteturais em minutos
- **ğŸ“Š Qualidade Consistente**: PadrÃµes mantidos automaticamente

---

**ğŸ’¡ Dica**: Use este arquivo como template para seus prÃ³prios projetos. A engenharia de prompt bem feita pode acelerar drasticamente o desenvolvimento!

--- 

#### Pode usar mas poh pelo menos me da o crÃ©dito, deu trabalho fazer isso aqui, olha os commits foram madrugadas a dentro para criar esse sistema.

### ğŸ”§ Teste Interativo
- **Interface Swagger**: Teste todos os endpoints diretamente no navegador
- **ValidaÃ§Ã£o de Headers**: Campos obrigatÃ³rios claramente identificados
- **Exemplos de Resposta**: Visualize o formato esperado de cada endpoint
- **CÃ³digos de Status**: DocumentaÃ§Ã£o completa de erros e sucessos

### ğŸ› **Modo Debug AvanÃ§ado**

O endpoint `/response/{document_id}` possui um modo debug especial para anÃ¡lise detalhada:

#### ğŸ” **AtivaÃ§Ã£o do Debug**
```bash
# Header obrigatÃ³rio para ativar debug
debug: 1
```

#### ğŸ“Š **InformaÃ§Ãµes Retornadas no Debug**

1. **ğŸ”¤ ConteÃºdo ExtraÃ­do (OCR/Parser)**
   - Texto completo extraÃ­do do documento
   - Ferramenta de extraÃ§Ã£o utilizada (Tesseract, PyPDF2, etc.)
   - InformaÃ§Ãµes do arquivo (nome, tipo, tamanho)
   - EstatÃ­sticas de caracteres extraÃ­dos

2. **ğŸ¤– Prompt Enviado para LLM**
   - Prompt original do usuÃ¡rio
   - Prompt completo construÃ­do (com contexto e instruÃ§Ãµes)
   - ConfiguraÃ§Ãµes de formataÃ§Ã£o solicitadas
   - Exemplos fornecidos
   - Provedor de AI utilizado (Ollama/Gemini)

3. **ğŸ’¬ Resposta Raw da LLM**
   - Resposta bruta/original da LLM
   - Resposta final formatada
   - EstatÃ­sticas de tokens/caracteres
   - ComparaÃ§Ã£o antes/depois da formataÃ§Ã£o

#### ğŸ¯ **Uso PrÃ¡tico do Debug**
- **Qualidade OCR**: Verificar se o texto foi extraÃ­do corretamente
- **Prompt Engineering**: Analisar se o prompt estÃ¡ bem construÃ­do
- **Troubleshooting**: Identificar onde estÃ¡ o problema (extraÃ§Ã£o, prompt ou modelo)
- **OtimizaÃ§Ã£o**: Melhorar prompts baseado na resposta raw

## ğŸ”— Endpoints da API