# ğŸ§  Document OCR LLM API

API escalÃ¡vel, segura e leve para anÃ¡lise de documentos com auxÃ­lio de modelos de linguagem locais (Ollama) e em nuvem (Google Gemini).

## ğŸ†• Novidades v1.3 (IntegraÃ§Ã£o Google Gemini API)
- âœ… **ğŸŒŸ Google Gemini API**: Suporte completo Ã  API Gemini do Google
- âœ… **ğŸ”€ Multi-Provider**: Alterne entre Ollama (local) e Gemini (nuvem)
- âœ… **ğŸš€ Modelos AvanÃ§ados**: Acesso aos modelos Gemini 2.0/2.5 mais recentes
- âœ… **ğŸ“Š Lista DinÃ¢mica**: Modelos sempre atualizados direto da API Google
- âœ… **ğŸ”‘ SeguranÃ§a**: Suporte a chaves API Gemini seguras
- âœ… **âš¡ Performance**: Modelos em nuvem de alta performance
- âœ… **ğŸ› ï¸ Compatibilidade**: Mesma interface para ambos os provedores

## ğŸ“‹ Ãndice

- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [Arquitetura](#-arquitetura)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [Como Usar](#-como-usar)
- [Endpoints](#-endpoints)
- [Provedores de IA](#-provedores-de-ia)
- [Teste](#-teste)
- [Escalabilidade](#-escalabilidade)
- [SeguranÃ§a](#-seguranÃ§a)
- [Monitoramento](#-monitoramento)
- [Troubleshooting](#-troubleshooting)

## ğŸš€ CaracterÃ­sticas

- **ğŸŒŸ Multi-Provider IA**: Suporte tanto para Ollama (local) quanto Google Gemini (nuvem)
- **ğŸš€ Smart Upload**: DetecÃ§Ã£o automÃ¡tica de tipo de arquivo e ferramenta apropriada
- **âš™ï¸ ConfiguraÃ§Ã£o CPU/GPU**: Controle dinÃ¢mico do modo de processamento Ollama  
- **OCR AvanÃ§ado**: Tesseract para extraÃ§Ã£o de texto de imagens (JPG, PNG)
- **Parsers MÃºltiplos**: PDF, DOCX, XLSX com detecÃ§Ã£o automÃ¡tica
- **LLM Local & Nuvem**: IntegraÃ§Ã£o com Ollama (local) e Google Gemini (nuvem)
- **Gerenciamento de Modelos**: Download e listagem via API (ambos provedores)
- **Sistema de Filas**: Processamento assÃ­ncrono com Celery + Redis
- **Modo Verbose**: Logs detalhados mostrando detecÃ§Ã£o automÃ¡tica
- **Auto-Limpeza**: RemoÃ§Ã£o automÃ¡tica de arquivos e dados antigos
- **Containerizado**: Docker com todas as dependÃªncias
- **EscalÃ¡vel**: Suporte a paralelizaÃ§Ã£o baseada no hardware
- **Monitoramento**: Logs estruturados e health checks avanÃ§ados

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI       â”‚    â”‚   Celery        â”‚    â”‚   Ollama        â”‚
â”‚   (API Server)  â”‚â”€â”€â”€â”€â”‚   (Workers)     â”‚â”€â”€â”€â”€â”‚   (Local LLM)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â–¼
         â”‚                       â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚              â”‚   Google        â”‚
         â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Gemini API    â”‚
         â”‚                                      â”‚   (Cloud LLM)   â”‚
         â–¼                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SQLite        â”‚    â”‚   Redis         â”‚    â”‚   Tesseract     â”‚
â”‚   (Database)    â”‚    â”‚   (Queue)       â”‚    â”‚   (OCR)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Processamento

1. **Upload**: Cliente envia arquivo + headers obrigatÃ³rios (incluindo AI-Provider)
2. **ValidaÃ§Ã£o**: VerificaÃ§Ã£o de API key, tipo e tamanho do arquivo, provider IA
3. **Fila de ExtraÃ§Ã£o**: Worker extrai texto (OCR/Parser)
4. **Fila de LLM**: Worker envia prompt para Ollama (local) ou Gemini (nuvem)
5. **Fila de FormataÃ§Ã£o**: Worker formata resposta final
6. **Resposta**: Cliente consulta resultado via API

## ğŸ› ï¸ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Docker 20.0+
- Docker Compose 2.0+
- 4GB+ RAM (recomendado 8GB+)
- 10GB+ espaÃ§o em disco
- **[OPCIONAL]** Chave API do Google Gemini (para usar modelos em nuvem)

### InstalaÃ§Ã£o via Docker

1. **Clone o repositÃ³rio**
```bash
git clone <repository-url>
cd MYELIN-OCR-LLM-LOCAL
```

2. **Configure o ambiente**
```bash
cp .env.example .env
# Edite o .env com suas configuraÃ§Ãµes
```

3. **Construa e execute o container**
```bash
# OpÃ§Ã£o 1: Docker Compose (recomendado)
docker-compose up --build

# OpÃ§Ã£o 2: Docker direto
docker build -t document-ocr-api .
docker run -p 8000:8000 -p 11434:11434 -v $(pwd)/uploads:/app/uploads document-ocr-api
```

4. **Aguarde a inicializaÃ§Ã£o**
```bash
# Verificar se estÃ¡ funcionando
curl http://localhost:8000/health
```

**Tempo de inicializaÃ§Ã£o**: ~5-10 minutos (download do modelo gemma3:1b)

## ğŸ¤– Provedores de IA

### ğŸ  Ollama (Local)
- **Vantagens**: Privacidade total, sem custos adicionais, funciona offline
- **Modelos**: gemma3:1b, qwen2:0.5b, llama3:8b, mistral:7b, etc.
- **Requisitos**: Hardware local com GPU/CPU adequados

### ğŸŒŸ Google Gemini (Nuvem)
- **Vantagens**: Modelos mais avanÃ§ados, sem requisitos de hardware
- **Modelos**: gemini-2.0-flash, gemini-2.5-pro-preview, gemini-1.5-pro, etc.
- **Requisitos**: Chave API Gemini (gratuita com limites)

### Como Obter Chave API Gemini

1. Acesse [Google AI Studio](https://aistudio.google.com/app/apikey)
2. FaÃ§a login com sua conta Google
3. Clique em "Get API Key"
4. Copie sua chave API
5. Use no header `Gemini-API-Key` das requisiÃ§Ãµes

## âš™ï¸ ConfiguraÃ§Ã£o

### Arquivo .env

```bash
# API Configuration
API_KEY=myelin-ocr-llm-2024-super-secret-key
DEBUG=False
HOST=0.0.0.0
PORT=8000

# Database Configuration
DATABASE_URL=sqlite:///./documents.db

# Redis Configuration
REDIS_URL=redis://localhost:6379/0

# Ollama Configuration (para uso local)
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=gemma3:1b

# File Upload Configuration
MAX_FILE_SIZE=50
ALLOWED_EXTENSIONS=pdf,jpg,jpeg,png,docx,xlsx,xls,doc

# Queue Configuration
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# Cleanup Configuration
CLEANUP_INTERVAL_HOURS=1
MAX_RECORD_AGE_HOURS=24

# Logging Configuration
LOG_LEVEL=INFO
LOG_ROTATION=10MB
```

## ğŸ“– Como Usar

### 1. ğŸ  Usando Ollama (Local)

```bash
# Processamento local com Ollama
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Verifique qual CNPJ existe nesse documento" \
  -H "Format-Response: [{\"CNPJ\": \"\"}]" \
  -H "Model: gemma3:1b" \
  -H "AI-Provider: ollama" \
  -F "file=@documento.pdf"
```

### 2. ğŸŒŸ Usando Google Gemini (Nuvem)

```bash
# Processamento em nuvem com Gemini
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Verifique qual CNPJ existe nesse documento e extraia todas as informaÃ§Ãµes da empresa" \
  -H "Format-Response: [{\"Dia da Leitura\": \"\"}]" \
  -H "Model: gemini-2.0-flash" \
  -H "AI-Provider: gemini" \
  -H "Gemini-API-Key: SUA_CHAVE_API_GEMINI" \
  -H "Example: [{\"Dia da Leitura\": \"31/12/9999\"}]" \
  -F "file=@documento.pdf"
```

**ğŸ› ï¸ DetecÃ§Ã£o AutomÃ¡tica:**
- `.jpg/.png` â†’ Tesseract OCR
- `.pdf` â†’ PyPDF2 Parser  
- `.docx` â†’ python-docx Parser
- `.xlsx` â†’ openpyxl Parser

**Resposta:**
```json
{
  "status": "success",
  "message": "Document uploaded and processing started",
  "document_id": 1,
  "filename": "documento.pdf",
  "ai_provider": "gemini",
  "model": "gemini-2.0-flash"
}
```

### 3. Verificar Status da Fila

```bash
curl -H "Key: myelin-ocr-llm-2024-super-secret-key" "http://localhost:8000/queue"
```

### 4. Obter Resposta

```bash
curl -H "Key: myelin-ocr-llm-2024-super-secret-key" "http://localhost:8000/response/1"
```

### 5. Gerenciamento de Modelos (NOVO!)

#### Listar Modelos DisponÃ­veis
```bash
curl -H "Key: myelin-ocr-llm-2024-super-secret-key" "http://localhost:8000/models/list"
```

**Resposta:**
```json
{
  "status": "success",
  "models": [
    {"name": "gemma3:1b", "status": "available"},
    {"name": "llama3:8b", "status": "available"}
  ],
  "total_models": 2
}
```

#### Baixar Novo Modelo
```bash
curl -X POST "http://localhost:8000/models/download" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Model-Name: llama3:8b"
```

**ğŸ¤– Modelos VÃ¡lidos e Recomendados:**
- **LLaMA 3 (Meta)**: `llama3:8b`, `llama3:70b`
- **Gemma 2 (Google)**: `gemma2:2b`, `gemma2:9b`, `gemma2:27b` 
- **Mistral AI**: `mistral:7b`, `mistral:instruct`
- **Qwen 2 (Alibaba)**: `qwen2:0.5b`, `qwen2:1.5b`, `qwen2:7b`
- **Phi-3 (Microsoft)**: `phi3:mini`, `phi3:medium`

ğŸ’¡ **RecomendaÃ§Ãµes de uso:**
- `qwen2:0.5b` - Mais rÃ¡pido (500MB)
- `gemma2:2b` - Equilibrado (1.6GB)  
- `llama3:8b` - Mais preciso (4.7GB)

âš ï¸ **Importante**: Download pode levar 5-30 minutos dependendo do modelo

### 6. âš™ï¸ ConfiguraÃ§Ã£o CPU/GPU (NOVO!)

#### Verificar Modo Atual
```bash
curl -H "Key: myelin-ocr-llm-2024-super-secret-key" "http://localhost:8000/config/compute"
```

**Resposta:**
```json
{
  "status": "success",
  "compute_mode": "cpu",
  "gpu_enabled": false,
  "cuda_devices": "",
  "config": {
    "OLLAMA_COMPUTE_MODE": "cpu",
    "OLLAMA_GPU_ENABLED": "0",
    "CUDA_VISIBLE_DEVICES": ""
  }
}
```

#### Alternar para GPU (Mais RÃ¡pido)
```bash
curl -X POST "http://localhost:8000/config/compute" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Compute-Mode: gpu"
```

#### Alternar para CPU (EconÃ´mico)
```bash
curl -X POST "http://localhost:8000/config/compute" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Compute-Mode: cpu"
```

**ğŸ”„ ConfiguraÃ§Ã£o Persistente:** As configuraÃ§Ãµes sÃ£o salvas no `.env` e aplicadas automaticamente.

### 7. Usando Modelos Diferentes

```bash
# Upload com LLaMA3:8b (mais preciso)
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: AnÃ¡lise detalhada deste documento" \
  -H "Format-Response: [{\"anÃ¡lise\": \"\", \"detalhes\": \"\"}]" \
  -H "Model: llama3:8b" \
  -F "file=@documento.pdf"
```

## ğŸ”— Endpoints

| MÃ©todo | Endpoint | DescriÃ§Ã£o | Headers ObrigatÃ³rios |
|--------|----------|-----------|---------------------|
| `POST` | `/upload` | ğŸš€ **SMART UPLOAD** - Auto-detecÃ§Ã£o | Key, Prompt, Format-Response, Model, AI-Provider |
| `GET` | `/queue` | Status da fila | Key |
| `GET` | `/response/{id}` | Resposta do documento | Key |
| `POST` | `/models/download` | Download de modelo Ollama | Key, Model-Name |
| `GET` | `/models/list` | Lista modelos Ollama | Key |
| `GET` | `/models/gemini` | **ğŸŒŸ NOVO** - Lista modelos Gemini | Key, Gemini-API-Key |
| `POST` | `/config/compute` | **ğŸ†• NOVO** - Configurar CPU/GPU | Key, Compute-Mode |
| `GET` | `/config/compute` | **ğŸ†• NOVO** - Ver modo atual | Key |
| `GET` | `/health` | Health check | - |
| `GET` | `/` | InformaÃ§Ãµes da API | - |

### Headers ObrigatÃ³rios

| Header | Exemplo | DescriÃ§Ã£o |
|--------|---------|-----------|
| `Key` | `myelin-ocr-llm-2024-super-secret-key` | Chave de autenticaÃ§Ã£o |
| `Prompt` | `"Extraia o CNPJ"` | Pergunta sobre o documento |
| `Format-Response` | `[{"CNPJ": ""}]` | Formato esperado da resposta |
| `Model` | `gemma3:1b` ou `gemini-2.0-flash` | Modelo Ollama ou Gemini a usar |
| `Example` | `[{"CNPJ": "12.345.678/0001-90"}]` | Exemplo de resposta (opcional) |
| `AI-Provider` | `ollama` ou `gemini` | Provider de IA a usar |
| `Gemini-API-Key` | `AIzaSy...` | Chave API Gemini (obrigatÃ³ria quando AI-Provider=gemini) |

### ğŸŒŸ Exemplos de Uso Gemini

#### Listar Modelos Gemini DisponÃ­veis
```bash
curl -X GET "http://localhost:8000/models/gemini" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Gemini-API-Key: SUA_CHAVE_API_GEMINI"
```

**Resposta:**
```json
{
  "status": "success",
  "provider": "gemini",
  "models": [
    {
      "name": "gemini-2.0-flash",
      "display_name": "Gemini 2.0 Flash",
      "description": "Latest multimodal model with next generation features",
      "input_token_limit": "1,048,576",
      "output_token_limit": "8,192"
    },
    {
      "name": "gemini-2.5-pro-preview",
      "display_name": "Gemini 2.5 Pro Preview", 
      "description": "Most powerful thinking model with maximum accuracy",
      "input_token_limit": "1,048,576",
      "output_token_limit": "65,536"
    }
  ],
  "total_models": 7,
  "documentation": "https://ai.google.dev/gemini-api/docs/models"
}
```

#### ComparaÃ§Ã£o de Modelos Recomendados

| Caso de Uso | Ollama (Local) | Gemini (Nuvem) | Vantagem |
|-------------|----------------|-----------------|----------|
| **Documentos Simples** | gemma3:1b | gemini-2.0-flash-lite | Local: Privacidade |
| **AnÃ¡lise Complexa** | llama3:8b | gemini-2.0-flash | Nuvem: Performance |
| **RaciocÃ­nio AvanÃ§ado** | llama3:70b | gemini-2.5-pro-preview | Nuvem: Capacidade |
| **Alto Volume** | qwen2:0.5b | gemini-1.5-flash-8b | Local: Sem limite de requests |
| **Processamento Offline** | Qualquer | âŒ NÃ£o disponÃ­vel | Local: Funcionamento offline |

### Status de Processamento

- `uploaded`: Arquivo recebido
- `text_extracted`: Texto extraÃ­do com sucesso
- `prompt_processed`: LLM processou o prompt
- `completed`: Processamento finalizado
- `error`: Erro durante processamento

## ğŸ§ª Teste

### Script de Teste Automatizado

```bash
# Windows
test_api.bat

# Linux/Mac
chmod +x test_api.sh
./test_api.sh
```

### Collection do Postman

1. **Importe os arquivos no Postman:**
   - `postman_collection.json` (Collection)
   - `postman_environment.json` (Environment)

2. **Configure o ambiente** "Document OCR LLM API - Local"

3. **Como usar:**
   - Execute "Health Check" primeiro
   - Execute "Upload Document (Image)" com arquivo `teste.jpg`
   - O `document_id` serÃ¡ preenchido automaticamente
   - Execute "Get Queue Status" para acompanhar
   - Execute "Get Document Response" para o resultado

### Testes Manuais

```bash
# 1. Health Check
curl http://localhost:8000/health

# 2. Upload com teste.jpg
curl -X POST "http://localhost:8000/upload" \
  -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  -H "Prompt: Descreva o que vocÃª vÃª na imagem" \
  -H "Format-Response: {\"descricao\": \"\"}" \
  -H "Model: gemma3:1b" \
  -H "AI-Provider: ollama" \
  -F "file=@teste.jpg"

# 3. Verificar status
curl -H "Key: myelin-ocr-llm-2024-super-secret-key" \
  "http://localhost:8000/queue"
```

## ğŸ“ˆ Escalabilidade

### ConfiguraÃ§Ã£o para MÃ¡quinas Modestas

```bash
# No .env
CELERY_WORKERS=1
CELERY_CONCURRENCY=1
MODEL=gemma3:1b
```

### ConfiguraÃ§Ã£o para MÃ¡quinas Potentes

```bash
# No .env
CELERY_WORKERS=4
CELERY_CONCURRENCY=4
MODEL=llama2:7b
```

### Escalonamento Manual

```bash
# Adicionar mais workers
docker exec -d <container> celery -A workers worker --concurrency=4

# Monitorar workers
docker exec <container> celery -A workers inspect active
```

## ğŸ”’ SeguranÃ§a

### ValidaÃ§Ãµes Implementadas

- âœ… AutenticaÃ§Ã£o via API Key
- âœ… ValidaÃ§Ã£o de tipos de arquivo
- âœ… LimitaÃ§Ã£o de tamanho de arquivo
- âœ… SanitizaÃ§Ã£o de inputs
- âœ… Rate limiting (via Celery)

### ConfiguraÃ§Ãµes de SeguranÃ§a

```bash
# Tipos de arquivo permitidos
ALLOWED_EXTENSIONS=pdf,jpg,jpeg,png,docx,xlsx

# Tamanho mÃ¡ximo
MAX_FILE_SIZE=50

# API Key forte
API_KEY=myelin-ocr-llm-2024-super-secret-key
```

## ğŸ“Š Monitoramento (Modo Verbose Ativo!)

### Logs Detalhados em Tempo Real

O sistema agora opera em **modo verbose** com logs detalhados de todo o processo:

```bash
# Ver logs em tempo real do container
docker logs <container_name> -f

# Logs especÃ­ficos por serviÃ§o (com modo verbose)
docker exec -it <container_name> tail -f /var/log/supervisor/fastapi.log     # FastAPI verbose
docker exec -it <container_name> tail -f /var/log/supervisor/celery_worker.log  # Celery debug
docker exec -it <container_name> tail -f /var/log/supervisor/ollama.log        # Ollama verbose

# Logs da aplicaÃ§Ã£o (dentro do container)
docker exec -it <container_name> tail -f /app/logs/app.log        # Logs gerais
docker exec -it <container_name> tail -f /app/logs/verbose.log    # Logs verbose especÃ­ficos
```

### ğŸ“‹ O que vocÃª verÃ¡ nos logs verbose:

#### ğŸ“¤ **Smart Upload Process**
```
ğŸ“¤ VERBOSE: Starting document upload process
ğŸ“ VERBOSE: Filename: teste.jpg
ğŸ¤– VERBOSE: Model requested: gemma3:1b
ğŸ“ VERBOSE: File size: 2.45 MB
ğŸ” VERBOSE: Auto-detected file type: JPG
ğŸ› ï¸ VERBOSE: Will use extraction tool: Tesseract OCR
ğŸ’¾ VERBOSE: Saving file to disk...
âœ… VERBOSE: File saved to: uploads/uuid_teste.jpg
ğŸ—„ï¸ VERBOSE: Creating database record...
ğŸš€ VERBOSE: Starting Celery task for document 123
```

#### ğŸ” **OCR Process**
```
ğŸ” VERBOSE: Starting text extraction from JPG file: uploads/uuid_teste.jpg
ğŸ–¼ï¸ VERBOSE: Using OCR (Tesseract) for image processing
âœ… VERBOSE: Text extraction completed. Extracted 847 characters
ğŸ“„ VERBOSE: Text preview: NOTA FISCAL ELETRÃ”NICA...
```

#### ğŸ¤– **LLM Process**
```
ğŸ¤– VERBOSE: Sending prompt to Ollama model 'gemma3:1b'
ğŸ“„ VERBOSE: Context length: 847 characters
â“ VERBOSE: Prompt: Verifique qual CNPJ existe nesse documento
ğŸ”— VERBOSE: Making request to http://localhost:11434/api/generate
âœ… VERBOSE: Ollama response received (234 chars)
ğŸ’¬ VERBOSE: Response preview: O CNPJ encontrado no documento Ã©...
â±ï¸ VERBOSE: Total duration: 3.45s
ğŸ”¢ VERBOSE: Prompt tokens: 298
ğŸ“Š VERBOSE: Response tokens: 87
```

#### âš™ï¸ **CPU/GPU Configuration**
```
ğŸ”„ VERBOSE: Switching Ollama compute mode to: GPU
ğŸš€ VERBOSE: GPU mode enabled - CUDA devices accessible
ğŸ”„ VERBOSE: Restarting Ollama service with GPU mode...
âœ… VERBOSE: Ollama restarted successfully in GPU mode
â„¹ï¸ VERBOSE: Current compute mode: GPU
```

### Logs Tradicionais

### Health Checks

```bash
# API Health
curl http://localhost:8000/health

# Ollama Health
curl http://localhost:11434/api/tags

# Redis Health
docker exec <container> redis-cli ping
```

### MÃ©tricas

```bash
# Status dos workers
docker exec <container> celery -A workers inspect stats

# Fila de tarefas
docker exec <container> celery -A workers inspect reserved

# Uso de recursos
docker stats <container_name>
```

## ğŸ”§ Troubleshooting

### Problemas Comuns

#### 1. Container nÃ£o inicia
```bash
# Verificar logs
docker logs <container_name>

# Verificar portas
netstat -tulpn | grep :8000
```

#### 2. Ollama nÃ£o responde
```bash
# Verificar se o modelo foi baixado
docker exec <container> ollama list

# Baixar modelo manualmente
docker exec <container> ollama pull gemma3:1b
```

#### 3. Workers nÃ£o processam
```bash
# Verificar Redis
docker exec <container> redis-cli ping

# Reiniciar workers
docker exec <container> supervisorctl restart celery_worker
```

#### 4. Erro de OCR
```bash
# Verificar Tesseract
docker exec <container> tesseract --version

# Testar OCR
docker exec <container> tesseract /app/teste.jpg stdout -l por
```

### Limpeza Manual

```bash
# Limpar arquivos antigos
docker exec <container> python3 -c "from utils import cleanup_old_files; cleanup_old_files()"

# Limpar banco de dados
docker exec <container> python3 -c "
from database import SessionLocal
from models import Document
from datetime import datetime, timedelta
db = SessionLocal()
cutoff = datetime.utcnow() - timedelta(hours=1)
deleted = db.query(Document).filter(Document.created_at < cutoff).delete()
db.commit()
print(f'Deleted {deleted} records')
"
```

## ğŸš€ URLs Importantes

- **API Principal**: http://localhost:8000
- **DocumentaÃ§Ã£o**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health
- **Ollama**: http://localhost:11434

## ğŸ“„ Arquivos de ConfiguraÃ§Ã£o

- `build_and_run.bat` - Script de build para Windows
- `test_api.bat` - Script de teste para Windows
- `postman_collection.json` - Collection do Postman
- `postman_environment.json` - Environment do Postman
- `.env` - ConfiguraÃ§Ãµes da aplicaÃ§Ã£o
- `docker-compose.yml` - OrquestraÃ§Ã£o de containers

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **GNU Affero General Public License v3.0 (AGPL-3.0)**.

### ğŸ†“ Uso Livre Permitido
- âœ… **Uso pessoal e educacional**
- âœ… **Pesquisa e desenvolvimento**
- âœ… **Projetos open source** (devem tambÃ©m usar licenÃ§a compatÃ­vel)
- âœ… **ContribuiÃ§Ãµes** para este projeto

### ğŸ¢ Uso Comercial
Para **uso comercial**, incluindo:
- ğŸ’¼ IntegraÃ§Ã£o em produtos comerciais
- ğŸŒ Oferecimento como serviÃ§o (SaaS)
- ğŸ’° Venda ou distribuiÃ§Ã£o comercial
- ğŸ­ Uso em ambiente empresarial com fins lucrativos

**Ã‰ necessÃ¡ria uma licenÃ§a comercial separada.** Entre em contato:

ğŸ“§ **Email**: carlosmagnosilvatavares@gmail.com  
ğŸ’¬ **LinkedIn**: https://www.linkedin.com/in/carlosmagnosilvatavares/
ğŸ“„ **Termos**: NegociÃ¡veis conforme o caso de uso


### âš–ï¸ Compliance AGPL
Ao usar este software sob AGPL-3.0, vocÃª deve:
- ğŸ“– **Manter avisos de copyright** em todos os arquivos
- ğŸ”“ **Disponibilizar cÃ³digo fonte** de qualquer modificaÃ§Ã£o
- ğŸ“‹ **Usar licenÃ§a compatÃ­vel** em projetos derivados
- ğŸŒ **Fornecer cÃ³digo fonte** mesmo para uso como serviÃ§o web

### ğŸ“‹ LicenÃ§as de Terceiros
Este projeto utiliza bibliotecas open source com suas respectivas licenÃ§as:
- **FastAPI**: MIT License
- **Tesseract OCR**: Apache 2.0 License  
- **Ollama**: MIT License
- **PostgreSQL**: PostgreSQL License
- **Redis**: BSD License

Para mais detalhes, consulte o arquivo [LICENSE](LICENSE).

### ğŸ¤ ContribuiÃ§Ãµes
ContribuiÃ§Ãµes sÃ£o bem-vindas! Ao contribuir, vocÃª concorda que suas contribuiÃ§Ãµes serÃ£o licenciadas sob os mesmos termos deste projeto.

Para contribuir:
1. FaÃ§a um fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

---

ğŸ¯ **Pronto para usar!** O sistema estÃ¡ configurado para rodar em mÃ¡quinas modestas e escalar conforme necessÃ¡rio.

ğŸ“œ **Licenciado sob AGPL-3.0** - Uso livre para projetos compatÃ­veis, licenÃ§a comercial para uso empresarial.

---

**ğŸ§  Powered by Myelin AI - Processamento inteligente de documentos**

## ğŸ¯ **Sobre o Desenvolvimento deste Sistema**

### ğŸ“ **Prompt-inicial-para-gerar-sistema-com-cursor-e-claude4.txt**

Este sistema foi **inteiramente gerado** atravÃ©s de engenharia de prompt avanÃ§ada Criado pelo engenheiro de dados Carlos Magno utilizando **Claude Sonnet 4** no **Cursor IDE**. O arquivo `Prompt-inicial-para-gerar-sistema-com-cursor-e-claude4.txt` contÃ©m o prompt original e atualizado que foi usado para criar toda a aplicaÃ§Ã£o.

#### ğŸ”„ **Como Funciona:**

1. **ğŸ“‹ Prompt Inicial**: O arquivo contÃ©m especificaÃ§Ãµes completas do sistema
2. **ğŸ¤– Claude Sonnet 4**: Interpreta e gera cÃ³digo baseado nas especificaÃ§Ãµes
3. **âš¡ Cursor IDE**: Facilita a implementaÃ§Ã£o e refinamento do cÃ³digo
4. **ğŸ”§ IteraÃ§Ãµes**: Melhorias contÃ­nuas atravÃ©s de prompts refinados

#### ğŸ—ï¸ **Arquitetura do Prompt:**

```
ğŸ“ Prompt Inicial v1.3
â”œâ”€â”€ ğŸ¯ Objetivo Principal
â”œâ”€â”€ ğŸ“¥ Requisitos Funcionais
â”‚   â”œâ”€â”€ Headers obrigatÃ³rios
â”‚   â”œâ”€â”€ Headers opcionais  
â”‚   â””â”€â”€ Multi-provider support
â”œâ”€â”€ ğŸ› ï¸ Requisitos TÃ©cnicos
â”‚   â”œâ”€â”€ Infraestrutura Docker
â”‚   â”œâ”€â”€ Sistema de filas
â”‚   â”œâ”€â”€ Gerenciamento de modelos
â”‚   â””â”€â”€ Banco de dados
â”œâ”€â”€ ğŸ“Š Endpoints Completos
â”‚   â”œâ”€â”€ Core endpoints
â”‚   â”œâ”€â”€ Gerenciamento de modelos
â”‚   â”œâ”€â”€ ConfiguraÃ§Ã£o avanÃ§ada
â”‚   â””â”€â”€ Monitoramento
â”œâ”€â”€ ğŸ§ª Testes e Exemplos
â”œâ”€â”€ ğŸ“ OrganizaÃ§Ã£o do projeto
â”œâ”€â”€ ğŸ›¡ï¸ SeguranÃ§a
â””â”€â”€ ğŸš€ Recursos avanÃ§ados
```

#### ğŸ“ˆ **EvoluÃ§Ã£o do Sistema:**

| VersÃ£o | Prompt Focus | Resultado |
|--------|--------------|-----------|
| **v1.0** | Sistema bÃ¡sico OCR + Ollama | Core functionality |
| **v1.1** | Smart upload + Auto-detection | File type detection |
| **v1.2** | CPU/GPU config + Model management | Performance optimization |
| **v1.3** | **Multi-provider AI + Gemini** | **Cloud + Local hybrid** |

#### ğŸ¨ **Metodologia de Desenvolvimento:**

1. **ğŸ“‹ EspecificaÃ§Ã£o Detalhada**
   - Requisitos funcionais claros
   - Exemplos de uso concretos
   - Estrutura de arquivos definida

2. **ğŸ¤– GeraÃ§Ã£o Assistida por IA**
   - Claude Sonnet 4 para lÃ³gica complexa
   - Cursor IDE para refinamentos
   - IteraÃ§Ãµes baseadas em feedback

3. **ğŸ”§ Refinamento ContÃ­nuo**
   - Testes em tempo real
   - Melhorias baseadas em uso
   - DocumentaÃ§Ã£o auto-atualizada

4. **ğŸ“Š ValidaÃ§Ã£o PrÃ¡tica**
   - Collection Postman completa
   - Testes automatizados
   - Logs detalhados para debug

#### ğŸ’¡ **Por que este MÃ©todo Ã© Eficaz:**

âœ… **EspecificaÃ§Ã£o Completa**: O prompt detalha cada aspecto do sistema
âœ… **ConsistÃªncia Arquitetural**: MantÃ©m padrÃµes em todo o cÃ³digo
âœ… **DocumentaÃ§Ã£o AutomÃ¡tica**: README e collection gerados automaticamente
âœ… **Testabilidade**: Inclui testes e validaÃ§Ãµes desde o inÃ­cio
âœ… **Escalabilidade**: Arquitetura pensada para crescimento
âœ… **Manutenibilidade**: CÃ³digo limpo e bem estruturado

#### ğŸ”„ **Como Usar o Prompt:**

1. **ğŸ“ Para Recriar o Sistema:**
   ```bash
   # Cole o conteÃºdo do arquivo no Claude Sonnet 4
   # Especifique ajustes desejados
   # Execute no Cursor IDE
   ```

2. **ğŸ”§ Para ModificaÃ§Ãµes:**
   ```bash
   # Edite seÃ§Ãµes especÃ­ficas do prompt
   # Mantenha a estrutura geral
   # Atualize exemplos conforme necessÃ¡rio
   ```

3. **ğŸ“ˆ Para Novas Funcionalidades:**
   ```bash
   # Adicione Ã  seÃ§Ã£o "Recursos AvanÃ§ados"
   # Especifique requisitos tÃ©cnicos
   # Inclua exemplos de teste
   ```

#### ğŸ¯ **BenefÃ­cios da Abordagem:**

| Vantagem | DescriÃ§Ã£o |
|----------|-----------|
| **âš¡ Velocidade** | Sistema completo em horas, nÃ£o semanas |
| **ğŸ“Š Qualidade** | PadrÃµes consistentes e best practices |
| **ğŸ”§ Flexibilidade** | FÃ¡cil modificaÃ§Ã£o via prompt updates |
| **ğŸ“š DocumentaÃ§Ã£o** | Auto-gerada e sempre atualizada |
| **ğŸ§ª Testabilidade** | Testes incluÃ­dos desde o design |
| **ğŸ”„ IteraÃ§Ã£o** | Melhorias rÃ¡pidas baseadas em feedback |

#### ğŸš€ **Futuro do Desenvolvimento:**

Esta metodologia representa o **futuro do desenvolvimento de software**:
- **ğŸ¤– IA como Copiloto AvanÃ§ado**: NÃ£o apenas sugestÃµes, mas arquitetura completa
- **ğŸ“ EspecificaÃ§Ã£o Declarativa**: Descrever "o que" ao invÃ©s de "como"
- **ğŸ”„ IteraÃ§Ã£o RÃ¡pida**: MudanÃ§as arquiteturais em minutos
- **ğŸ“Š Qualidade Consistente**: PadrÃµes mantidos automaticamente

---

**ğŸ’¡ Dica**: Use este arquivo como template para seus prÃ³prios projetos. A engenharia de prompt bem feita pode acelerar drasticamente o desenvolvimento!

--- 

#### Pode usar mas poh pelo menos me da o crÃ©dito, deu trabalho fazer isso aqui, olha os commits foram madrugadas a dentro para criar esse sistema.